<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ml on Jinkyu Koo</title>
    <link>https://helix979.github.io/jkoo/tags/ml/index.xml</link>
    <description>Recent content in Ml on Jinkyu Koo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jinkyu Koo</copyright>
    <atom:link href="/jkoo/tags/ml/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Logistic Regression</title>
      <link>https://helix979.github.io/jkoo/post/ml-logistic/</link>
      <pubDate>Thu, 09 Feb 2017 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-logistic/</guid>
      <description>

&lt;p&gt;Logistic regression is a classification method for analyzing a set of
data in which there are one or more independent variables (inputs) that
determine a binary outcome $0$ or $1$.&lt;/p&gt;

&lt;h3 id=&#34;hypothesis-function&#34;&gt;Hypothesis function&lt;/h3&gt;

&lt;p&gt;Suppose that we are given $M$ data samples, denoting the $m$-th
observation pair of an input vector and an output by
$\mathbf{x}_{m} = \begin{bmatrix} x_0 &amp;amp; x_1 &amp;amp; \cdots &amp;amp; x_n \end{bmatrix}^{T} \in \mathbb{R}^{n+1}$
and $y_{m} \in {0,1}$, respectively. We call $\mathbf{x}_{m}$ a
positive sample if $y_{m}=1$ and a negative sample if $y_{m}=0$.&lt;/p&gt;

&lt;p&gt;Now, consider $P(y_{m} \vert \mathbf{x}_{m})$, a conditional probability
of $y_{m}$ given $\mathbf{x}_{m}$. The &lt;strong&gt;hypothesis function&lt;/strong&gt;
$h_{\mathbf{w}}(\mathbf{x}_{m})$ of logistic regression models the
conditional probability of a positive sample (&lt;em&gt;i.e.&lt;/em&gt;, $y_{m}=1$) as&lt;/p&gt;

&lt;p&gt;$$
P(y_{m}=1 \vert \mathbf{x}_{m})=h_{\mathbf{w}}(\mathbf{x}_{m}).
$$&lt;/p&gt;

&lt;p&gt;Here, the hypothesis function $h_{\mathbf{w}}(\mathbf{x}_{m})$ is
defined using the &lt;strong&gt;sigmoid function&lt;/strong&gt; $s(z)=\frac{1}{1 + e^{-z}}$
as follows:&lt;/p&gt;

&lt;p&gt;$$
h_{\mathbf{w}}(\mathbf{x}_{m})=s(\mathbf{w}^T \mathbf{x}_{m})=\frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}_{m}}}.
$$&lt;/p&gt;

&lt;p&gt;As shown in Figure 1, the sigmoid function $s(z)$ is a
‘S’-shape curve that converges to 1 as $z \rightarrow \infty$ and 0 as
$z \rightarrow -\infty$. Since $h_{\mathbf{w}}(\mathbf{x}_{m})$ attempts
to model a probability, the property of the sigmoid function $s(z)$ that
is bounded between 0 and 1 is a good fit for our purpose.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/sigmoid.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;sigmoid function $s(z)=\frac{1}{1 &amp;#43; e^{-z}}$.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;What is next is to choose $\mathbf{w}$ well so that we can have a high
value of $h_{\mathbf{w}}(\mathbf{x}_{m})$ for $\mathbf{x}_{m}$ that
corresponds to $y_{m}=1$, and a low value for $\mathbf{x}_{m}$ that
corresponds to $y_{m}=0$.&lt;/p&gt;

&lt;h3 id=&#34;cost-function&#34;&gt;Cost function&lt;/h3&gt;

&lt;p&gt;Since $P(y_{m}=1 \vert \mathbf{x}_{m})=h_{\mathbf{w}}(\mathbf{x}_{m})$
by our definition, we have&lt;/p&gt;

&lt;p&gt;$$
P(y_{m}=0 \vert \mathbf{x}_{m})=1-h_{\mathbf{w}}(\mathbf{x}_{m}).
$$&lt;/p&gt;

&lt;p&gt;For all the observations in the training set, the likelihood
$l(\mathbf{w})$ can then be written as&lt;/p&gt;

&lt;p&gt;$$
l(\mathbf{w})=\prod_{m=1}^{M} h_{\mathbf{w}}(\mathbf{x}_{m})^{y_{m}} ( 1 - h_{\mathbf{w}}(\mathbf{x}_{m}) )^{1-{y_{m}}}.
\tag{logi:1}\label{eq:lr_likelihood}
$$&lt;/p&gt;

&lt;p&gt;Logistic regression chooses the
parameter $\mathbf{w}$ that maximizes the likelihood of observing the
samples, represented in \eqref{eq:lr_likelihood}. Thus, we define the
&lt;strong&gt;cost function&lt;/strong&gt; $J(\mathbf{w})$ as&lt;/p&gt;

&lt;p&gt;$$\scriptsize
\begin{aligned}
J(\mathbf{w})
&amp;amp;= -\frac{1}{M}\log l(\mathbf{w}) \\&lt;br /&gt;
&amp;amp;= -\frac{1}{M}\sum_{m=1}^{M} \left( y_{m} \log h_{\mathbf{w}}(\mathbf{x}_{m}) +  ( 1-{y_{m}}) \log( 1 - h_{\mathbf{w}}(\mathbf{x}_{m}) ) \right).
\end{aligned}\tag{logi:2}\label{eq:lr_cost}
$$&lt;/p&gt;

&lt;p&gt;Note that minimizing \eqref{eq:lr_cost} is equivalent to maximizing
\eqref{eq:lr_likelihood}.&lt;/p&gt;

&lt;h3 id=&#34;learning&#34;&gt;Learning&lt;/h3&gt;

&lt;p&gt;The solution $\mathbf{w}^{*}$ of logistic regression can be obtained by
minimizing the cost function in \eqref{eq:lr_cost}. Again, such a
minimization can be done by using the gradient descent. Hence, we first
obtain the partial derivatives of $J(\mathbf{w})$ as follows:&lt;/p&gt;

&lt;p&gt;$$\tiny
\begin{aligned}
&amp;amp;\frac{\partial J(\mathbf{w})}{\partial w_j} \\&lt;br /&gt;
&amp;amp;=-\frac{1}{M} \sum_{m=1}^M \left( y_{m} \frac{1}{s(\mathbf{w}^{T} \mathbf{x}_{m})} - (1 - y_{m})\frac{1}{1-s(\mathbf{w}^{T} \mathbf{x}_{m})} \right) \frac{\partial}{\partial w_j}s(\mathbf{w}^{T} \mathbf{x}_{m}) \\&lt;br /&gt;
&amp;amp;=-\frac{1}{M} \sum_{m=1}^M \left( y_{m} \frac{1}{s(\mathbf{w}^{T} \mathbf{x}_{m})} - (1 - y_{m})\frac{1}{1-s(\mathbf{w}^{T} \mathbf{x}_{m})} \right) s(\mathbf{w}^{T} \mathbf{x}_{m})(1-s(\mathbf{w}^{T} \mathbf{x}_{m}))\frac{\partial}{\partial w_j}\mathbf{w}^{T} \mathbf{x}_{m}\\&lt;br /&gt;
&amp;amp;=-\frac{1}{M} \sum_{m=1}^M \left( y_{m} (1-s(\mathbf{w}^{T} \mathbf{x}_{m})) - (1 - y_{m})s(\mathbf{w}^{T} \mathbf{x}_{m}) \right)x_{m,j} \\&lt;br /&gt;
&amp;amp;=\frac{1}{M} \sum_{m=1}^M (h_{\mathbf{w}} (\mathbf{x}_{m})- y_{m})x_{m,j}.
\end{aligned}\label{eq:logistic_derivative}
$$&lt;/p&gt;

&lt;p&gt;It is worth noting that
the sigmoid function $s(z)$ has an easy-to-calculate derivative:
$$\begin{aligned}
\frac{ds(z)}{dz}=s(z)(1-s(z)).\end{aligned}$$ Now, the gradient descent
equations for logistic regression are&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
w_{j}^{(t+1)}
&amp;amp;= w_{j}^{(t)}-\frac{\alpha}{M} \sum_{m=1}^{M} (h_{\mathbf{w}^{(t)}} (\mathbf{x}_{m})- y_{m})x_{m,j} \\&lt;br /&gt;
&amp;amp;= w_{j}^{(t)}-\frac{\alpha}{M} \sum_{m=1}^{M} \left(\frac{1}{1 + e^{-\mathbf{w}^{(t)T} \mathbf{x}_{m}}}- y_{m} \right)x_{m,j}
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;for all $j$.&lt;/p&gt;

&lt;h3 id=&#34;prediction&#34;&gt;Prediction&lt;/h3&gt;

&lt;p&gt;Given a new input vector
$\mathbf{x} = \begin{bmatrix} 1 &amp;amp; x_1 &amp;amp; \ldots &amp;amp; x_n \end{bmatrix}^{T}$,
we should predict $y$ in the following rule.&lt;/p&gt;

&lt;p&gt;$$
y = \left\{
\begin{matrix}
1 &amp;amp; \text{if } h_{\mathbf{w}^{*}} (\mathbf{x}) \ge 0.5,\\&lt;br /&gt;
0 &amp;amp; \text{if } h_{\mathbf{w}^{*}} (\mathbf{x}) &amp;lt; 0.5.\\&lt;br /&gt;
\end{matrix} \right.
\tag{logi:3}\label{eq:lr_decision}
$$&lt;/p&gt;

&lt;p&gt;That way we can achieve the minimum error rate in classification. (Why? Refer to Bayes decision rule for detailed
reason.)&lt;/p&gt;

&lt;p&gt;Since
$h_{\mathbf{w}^{*}} (\mathbf{x})=s\left(\mathbf{w}^{*T} \mathbf{x}\right)$
and $s(z) \ge 0.5 $ when $z \ge 0$, the decision rule in
\eqref{eq:lr_decision} is equivalent to&lt;/p&gt;

&lt;p&gt;$$
y = \left\{
\begin{matrix}
1 &amp;amp; \text{if } \mathbf{w}^{*T} \mathbf{x} \ge 0,\\&lt;br /&gt;
0 &amp;amp; \text{if } \mathbf{w}^{*T} \mathbf{x} &amp;lt; 0.\\&lt;br /&gt;
\end{matrix}
\right.
\label{eq:lr_decision2}
$$&lt;/p&gt;

&lt;p&gt;Note that the solution to the linear equation
$\mathbf{w}^{*T} \mathbf{x}=0$ is the decision boundary separating the
two predicted classes. Figure 2 shows an example
where the decision boundary is a line.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/logistic.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Decision boundary (the blue line) learned by logistic regression separates the data samples, where the green dots and red x’s belong to a different class.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;h3 id=&#34;practice&#34;&gt;Practice&lt;/h3&gt;

&lt;p&gt;Figure 2 can be obtained by the following.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

n_data = 50
r_x = 10
noise_std = 5

class0 = []
class1 = []
x = None
y = None
#--------------------------------------------------------
def create_data():
    global class0, class1, x, y

    slope_1, y_intercept_1 = 1, 5
    slope_2, y_intercept_2 = 1, 0

    for i in range(n_data):
        x_1 = np.random.rand() * r_x
        x_2 = slope_1 * x_1 + y_intercept_1 + (np.random.rand() - 0.5) * noise_std
        class0.append([x_1, x_2, 0])

        x_1 = np.random.rand() * r_x
        x_2 = slope_2 * x_1 + y_intercept_2 + (np.random.rand() - 0.5) * noise_std
        class1.append([x_1, x_2, 1])

    dataset = sorted(class0+class1, key=lambda data: data[0])
    x1, x2, y = zip(*dataset)
    x = np.vstack((np.float32(np.array(x1)), np.float32(np.array(x2))))
    y = np.array(y)
#--------------------------------------------------------
def draw(w, w_0):
    xr = np.arange(0,r_x, 0.1)
    yr  = -(w[0][0] * xr + w_0[0] )/w[0][1]

    for x1, x2, y in class0:
        plt.plot(x1, x2, &#39;go&#39;, markersize=10)

    for x1, x2, y in class1:
        plt.plot(x1, x2, &#39;rx&#39;, markersize=10, mew=2)

    plt.plot(xr, yr, lw=2)
    plt.xlim(0, r_x)
    plt.xlabel(&#39;$x_1$&#39;, fontsize=20)
    plt.ylabel(&#39;$x_2$&#39;, fontsize=20)
    plt.gcf().subplots_adjust(bottom=0.15)
    plt.savefig(&#39;logistic.pdf&#39;)
#--------------------------------------------------------

# Create data
# x of shape [2, n_data], y of shape [1, n_data]
create_data()

# Write a model
w = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))
w_0 = tf.Variable(tf.random_uniform([1], -1.0, 1.0))
z = tf.matmul(w, x) + w_0   # w_0 is broadcasted

y_hat = 1 / (1+tf.exp(-z))
cost = -tf.reduce_sum(y*tf.log(y_hat) + (1-y)*(tf.log(1-y_hat))) / len(y)

optimizer = tf.train.GradientDescentOptimizer(0.001)
train = optimizer.minimize(cost)

sess=tf.InteractiveSession()

# Initialize variables
init = tf.initialize_all_variables()
sess.run(init)

# Training
step = 0
while 1:
    try:
        step += 1
        sess.run(train)
        if step % 100 == 0:
            print step, sess.run(cost), sess.run(w), sess.run(w_0)

    # Ctrl+c will stop training
    except KeyboardInterrupt:
        break

# Plot the sample data and decision boundary
draw(sess.run(w), sess.run(w_0))

sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;one-versus-rest-handling-multiclass-classification&#34;&gt;One versus rest: handling multiclass classification&lt;/h3&gt;

&lt;p&gt;What if there are more than two classes to classify? What is called the
one-versus-rest strategy is a way of extending any binary classifier
(including the Logistic regression classifier) to handle multiclass
classification problems. The one-versus-rest strategy changes a
multiclass classification into multiple binary classifications, training
a single classifier per class. The data samples that belongs to a class
are treated as positive samples of the class and all others as negative
samples.&lt;/p&gt;

&lt;p&gt;Consider $y_m={1,2,\ldots,K}$, &lt;em&gt;i.e.&lt;/em&gt;, we have to decide one out
of $K$ classes. Using the Logistic regression as an example, we can make
a binary classifier for the class of $y_m=k$ with a corresponding
parameter vector $\mathbf{w}_k \in \mathbb{R}^{n+1}$ as&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
\mathbf{h}_{\mathbf{w}_k}(\mathbf{x}_{m}) =\frac{1}{1 + e^{-\mathbf{w}_k^T \mathbf{x}_{m}}}  \text{ for } k=1,2,\ldots,K.
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;The hypothesis function
$\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m}) \in \mathbb{R}^{K}$ of the
multiclass classification problem can then be expressed as
$$\begin{aligned}
\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m}) =
\begin{bmatrix}
\mathbf{h}_{\mathbf{w}_1}(\mathbf{x}_{m}) \\&lt;br /&gt;
\mathbf{h}_{\mathbf{w}_2}(\mathbf{x}_{m}) \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
\mathbf{h}_{\mathbf{w}_K}(\mathbf{x}_{m}) \\&lt;br /&gt;
\end{bmatrix},\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;where $\mathbf{W}$ is a parameter matrix
given as $$\begin{aligned}
\mathbf{W} =
\begin{bmatrix} \mathbf{w}_1 &amp;amp; \mathbf{w}_2 &amp;amp; \cdots &amp;amp; \mathbf{w}_K \end{bmatrix}.\end{aligned}$$
Note that for instance, when $y_m=1$, the first element of
$\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m})$, namely
$\mathbf{h}_{\mathbf{w}_1}(\mathbf{x}_{m})$, should be high and the
other elements should be low, since samples of the class $y_m=1$ are
treated as positives only for the class $y_m=1$ and as negatives for the
classes $y_m=2,3,\ldots, K$. Roughly speaking, this is achieved by
choosing $\mathbf{W}$ in such a way that
$\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m})$ is made as close to
$\mathbf{e}_k$ as possible when $y_m=k$, where $\mathbf{e}_k$ is the
$k$-th standard basis vector in $\mathbb{R}^{K}$, that is,
$$\begin{aligned}
\mathbf{e}_1 = \begin{bmatrix} 1 \\&lt;br /&gt;
0 \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
0
\end{bmatrix},
\mathbf{e}_2 = \begin{bmatrix} 0 \\&lt;br /&gt;
1 \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
0
\end{bmatrix},
\ldots,
\mathbf{e}_K = \begin{bmatrix} 0 \\&lt;br /&gt;
0 \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
1
\end{bmatrix}.
\label{eq:std_basis}\end{aligned}$$ Formally, the optimal $\mathbf{W}$
is chosen when we minimize the following cost function:&lt;/p&gt;

&lt;p&gt;$$\small
\begin{aligned}
J(\mathbf{W})
= -\frac{1}{M}\sum_{m=1}^{M} \sum_{k=1}^{K}
\left( \mathbf{1}_{m}(k) \log h_{\mathbf{w}_k}(\mathbf{x}_{m}) +  ( 1-\mathbf{1}_{m}(k)) \log( 1 - h_{\mathbf{w}_k}(\mathbf{x}_{m}) ) \right),
\label{eq:logistic_cost_multi}\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;where $\mathbf{1}_{m}(k)$
is an indicator function, defined as&lt;/p&gt;

&lt;p&gt;$$\begin{aligned}
\mathbf{1}_{m}(k) =
\left\{
\begin{matrix}
1 &amp;amp; \text{if } y_{m}=k,\\&lt;br /&gt;
0 &amp;amp; \text{if } y_{m} \neq k.
\end{matrix} \right.
\label{eq:indicator}\end{aligned}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://helix979.github.io/jkoo/post/ml-linear_regression/</link>
      <pubDate>Thu, 02 Feb 2017 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-linear_regression/</guid>
      <description>

&lt;p&gt;Linear regression is a means of modeling the relationship between one or
more independent variables (inputs) and a single dependent variable (an
output) by fitting a linear equation to observed data.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;hypothesis-function&#34;&gt;Hypothesis function&lt;/h2&gt;

&lt;p&gt;Given the $m$-th observation with inputs $\{ x_{m,1}, x_{m,2}, \ldots, x_{m,n} \}$
where
$x_{m,j} \in \mathbb{R}, \forall j$ and an output $y_m \in \mathbb{R}$,
we define an input vector as&lt;/p&gt;

&lt;p&gt;$$
\mathbf{x}_m=
\begin{bmatrix}
x_{m,0} \\&lt;br /&gt;
x_{m,1} \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
x_{m,n}
\end{bmatrix} \in \mathbb{R}^{n+1},
\label{eq:input_vector}
$$&lt;/p&gt;

&lt;p&gt;where we always set $x_{m,0}=1$, which is called a &lt;strong&gt;bias input&lt;/strong&gt; and considered here for notational
convenience. The goal of linear regression is to find an estimate
$\hat{y}_m=h_{\mathbf{w}}(\mathbf{x}_{m})$ of the output $y_{m}$ that is
of the form:&lt;/p&gt;

&lt;p&gt;$$
h_{\mathbf{w}}(\mathbf{x}_{m})=\mathbf{w}^T \mathbf{x}_{m}=w_0 + w_1 x_{m,1} + \ldots +w_n x_{m,n}.
$$&lt;/p&gt;

&lt;p&gt;We call $h_{\mathbf{w}}(\cdot)$ a &lt;strong&gt;hypothesis function&lt;/strong&gt;. By well
choosing the value of $\mathbf{w}$, we want
$h_{\mathbf{w}}(\mathbf{x}_{m})$ as close to $y_{m}$ as possible for
$m=1,2,\ldots,M$. Again, $M$ is the number of observations in the
training set.&lt;/p&gt;

&lt;p&gt;Note that when $n=1$, the hypothesis function is represented as&lt;/p&gt;

&lt;p&gt;$$
h_{\mathbf{w}}(\mathbf{x}_{m})=w_0 + w_1 x_{m,1},
$$&lt;/p&gt;

&lt;p&gt;which is the form of a straight line. Thus, all we need to do here is to find the slope
$w_1$ and the $y$-intercept $w_0$ of a straight line that fits best to
given points ${(x_{m,1}, y_{m})}_{m=1}^{M}$. This case is called
simple linear regression. Figure 1 shows an example of the simple linear regression.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/simple_linear.png&#34; alt=&#34;The blue dots represent the training data $\{(x_{m,1}, y_{m})\}_{m=1}^{M}$. The result of the simple linear regression is the red line&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Simple linear regression.&lt;/h4&gt;
        &lt;p&gt;
        The blue dots represent the training data $\{(x_{m,1}, y_{m})\}_{m=1}^{M}$. The result of the simple linear regression is the red line
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;cost-function&#34;&gt;Cost function&lt;/h2&gt;

&lt;p&gt;We need a measure of how &amp;ldquo;well&amp;rdquo; we have selected the value of
$\mathbf{w}$. To this end, the &lt;strong&gt;cost function&lt;/strong&gt; $J(\mathbf{w})$ can
be defined as&lt;/p&gt;

&lt;p&gt;$$
J(\mathbf{w})=\frac{1}{M} \sum_{m=1}^{M} \left(h_{\mathbf{w}}(\mathbf{x}_{m})-y_{m} \right)^{2}.
\tag{lr:1}\label{eq:mse}
$$&lt;/p&gt;

&lt;p&gt;Saying the value of $h_{\mathbf{w}}(\mathbf{x}_{m})-y_{m}$ is an error, the cost function
above is the mean of squared errors. Then, the best value of
$\mathbf{w}$, &lt;em&gt;i.e.&lt;/em&gt;, the solution $\mathbf{w}^{*}$, is chosen as the
one that minimizes the cost function $J(\mathbf{w})$ in \eqref{eq:mse}.
Such a solution is said to be optimal in the sense of minimizing mean-squared errors (MMSE).&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;learning&#34;&gt;Learning&lt;/h2&gt;

&lt;p&gt;Formally, the solution $\mathbf{w}^{*}$ is obtained as
$$
\mathbf{w}^{*}=\arg \min_{\mathbf{w}} J(\mathbf{w}).
$$&lt;/p&gt;

&lt;p&gt;This can be solved numerically by using the gradient descent. Since
$$
J(\mathbf{w})=\frac{1}{M} \sum_{m=1}^{M} \left(w_0 + w_1 x_{m,1} + \ldots +w_n x_{m,n}-y_{m} \right)^{2},
$$&lt;/p&gt;

&lt;p&gt;we have
$$
\frac{\partial}{\partial w_j} J(\mathbf{w})= \frac{2}{M} \sum_{m=1}^{M} \left(w_0 + w_1 x_{m,1} + \ldots +w_n x_{m,n}-y_{m} \right)x_{m,j}.
$$
Therefore, from &lt;a href=&#34;https://helix979.github.io/jkoo/jkoo/post/ml-optimization/#mjx-eqn-eqopt&#34; target=&#34;_blank&#34;&gt;(opt.1)&lt;/a&gt;, the gradient descent equations for
linear regression are&lt;/p&gt;

&lt;p&gt;$$
w_{j}^{(t+1)}=w_{j}^{(t)}-\alpha\frac{2}{M} \sum_{m=1}^{M} \left(w_0^{(t)} + w_1^{(t)} x_{m,1} + \ldots +w_n^{(t)} x_{m,n}-y_{m} \right)x_{m,j}
$$&lt;/p&gt;

&lt;p&gt;for all $j$.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;prediction&#34;&gt;Prediction&lt;/h2&gt;

&lt;p&gt;After we have found the solution $\mathbf{w}^{*}$, if an additional input vector
$\mathbf{x} = \begin{bmatrix} 1 &amp;amp; x_1 &amp;amp; \ldots &amp;amp; x_n \end{bmatrix}^{T}$
is given, its corresponding output $y$ can be predicted as follows:&lt;/p&gt;

&lt;p&gt;$$
y=h_{\mathbf{w}^{*}}(\mathbf{x})=w_0^{*} + w_1^{*} x_1 + \ldots +w_n^{*} x_n = \mathbf{w}^{*T} \mathbf{x}.
$$&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;practice&#34;&gt;Practice&lt;/h2&gt;

&lt;p&gt;The following code show an example of the simple lienar gression.
Data used for training is created by adding noises around the straight line of y-intercept=10 and slope=1.
The output would be like Figure 2.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

n_data = 100
r_x = 100
noise_std = 50
x = None
y = None
y_ideal = None
#----------------------------------------------------------------
def create_data():
    global x, y, y_ideal
    y_intercept = 10
    slope = 1
    x = np.float32(np.random.rand(n_data)) * r_x
    x = np.sort(x)
    noise = (np.float32(np.random.rand(n_data)) - 0.5) * noise_std
    y_ideal = slope * x + y_intercept
    y = y_ideal + noise
#----------------------------------------------------------------
def draw(w1, w0):
    y_hat = w1 * x + w0

    plt.plot(x[:], y_ideal[:], &#39;r--&#39;, \
             x[:], y[:], &#39;b.&#39;,\
             x[:], y_hat[:], &#39;g--&#39;)

    plt.xlim(0, r_x)
    plt.legend([&#39;ideal&#39;, &#39;data&#39;, &#39;regression&#39;], loc=&#39;best&#39;)
    plt.savefig(&#39;linear_one.pdf&#39;)
#----------------------------------------------------------------

# Create data for training
create_data()

# Model
w_1 = tf.Variable(np.random.rand())
w_0 = tf.Variable(np.random.rand())
m = w_1 * x
h = m + w_0   # Note w_0 is broadcasted to be the same shape as m

J = tf.reduce_mean(tf.square(h - y))
optimizer = tf.train.GradientDescentOptimizer(0.0001)
train = optimizer.minimize(J)

init = tf.initialize_all_variables()
sess=tf.InteractiveSession()
sess.run(init)

# Learning
step = 0
while 1:
    try:
        step += 1
        sess.run(train)
        if step % 100 == 0:
            print step, J.eval(), w_1.eval(), w_0.eval()

    # Ctrl+c will stop training
    except KeyboardInterrupt:
        break

# Plot the result
draw(w_1.eval(), w_0.eval())

sess.close()
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/linear_one.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Simple linear gression&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;The following code is doing the similar thing as above, but this time we consider the case of $n=2$.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D

N_data = 20
R_x = 100
noise_std = 100
xs = None
ys = None
zs = None
zs_ideal = None
X = None
Y = None
#----------------------------------------------------------------
def create_data():
    global xs, ys, zs, zs_ideal, X, Y
    slope_x = -2
    slope_y = 1
    z_intercept = 4
    x = np.random.rand(N_data) * R_x
    x = np.sort(x)
    y = np.random.rand(N_data) * R_x
    y = np.sort(y)
    X, Y = np.meshgrid(x, y)
    zf = lambda x, y: slope_x * x + slope_y * y + z_intercept
    xs = np.float32(np.ravel(X))
    ys = np.float32(np.ravel(Y))
    zs_ideal = np.array([zf(x,y) for x,y in zip(xs, ys)])
    zs = zs_ideal + (np.float32(np.random.rand(len(zs_ideal))) - 0.5) * noise_std
#----------------------------------------------------------------
def draw(w_est, w_0_est):
    zf_est = lambda x, y: w_est[0][0] * x + w_est[0][1] * y + w_0_est
    zs_est = np.array([zf_est(x,y) for x,y in zip(xs, ys)])
    Z_est = zs_est.reshape(X.shape)

    fig = plt.figure()
    ax = fig.gca(projection=&#39;3d&#39;)
    ax.plot_wireframe(X, Y, Z_est)

    for x,y,z in zip(xs, ys, zs[:]):
        ax.scatter(x,y,z, c=&#39;r&#39;, marker=&#39;.&#39;, s=20)

    ax.set_xlabel(&#39;x_1&#39;)
    ax.set_ylabel(&#39;x_2&#39;)
    ax.set_zlabel(&#39;z&#39;)

    plt.show()
#----------------------------------------------------------------

# Create data for training
create_data()

# Model
w_0 = tf.Variable(np.float32(np.random.rand()))
w = tf.Variable(np.float32(np.random.rand(1,2)))
h = tf.matmul(w, np.stack((xs,ys)) ) + w_0
loss = tf.reduce_mean(tf.square(h - zs))
optimizer = tf.train.GradientDescentOptimizer(0.0001)
train = optimizer.minimize(loss)

init = tf.initialize_all_variables()
sess=tf.InteractiveSession()
sess.run(init)

# Learning
step = 0
while 1:
    try:
        step += 1
        sess.run(train)
        if step % 100 == 0:
            print step, loss.eval(), w.eval()[0][0], w.eval()[0][1], w_0.eval()

    # Ctrl+c will stop training
    except KeyboardInterrupt:
        break

# Plot the result
draw(w.eval(), w_0.eval())

sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below is what we can obtained from the code.

&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/linear_multiple.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Linear regression ($n=2$)&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimization</title>
      <link>https://helix979.github.io/jkoo/post/ml-optimization/</link>
      <pubDate>Tue, 31 Jan 2017 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-optimization/</guid>
      <description>

&lt;p&gt;Machine learning often ends up with a mathematical optimization problem,
typically minimizing a cost function $J(\mathbf{w})$ for a given parameter $\mathbf{w} = \begin{bmatrix} w_0 &amp;amp; w_1 &amp;amp; \cdots &amp;amp; w_n \end{bmatrix}^{T} \in \mathbb{R}^{n+1}$ that has the form:&lt;/p&gt;

&lt;p&gt;$$
J(\mathbf{w})=\frac{1}{M} \sum_{m=1}^{M} J_m(\mathbf{w}).
\tag{opt:1}\label{eq:opt}
$$&lt;/p&gt;

&lt;p&gt;Here, $J_m(\mathbf{w})$ is the cost associated with the
$m$-th observation in a training set, and $M$ denotes the total number
of observations. Thus, one can say that given the parameter
$\mathbf{w}$, the cost function $J(\mathbf{w})$ represents the average
cost over all observations in the training set. Among all the possible
parameters, the one that results in the smallest value of the cost
function is called the &lt;strong&gt;minimizer, or solution&lt;/strong&gt;, and we denote it by $\mathbf{w}^{*}$.
The term &amp;ldquo;learning&amp;rdquo; in the machine learning means a
procedure for finding the solution $\mathbf{w}^{*}$.&lt;/p&gt;

&lt;h2 id=&#34;gradient-descent&#34;&gt;&lt;strong&gt;Gradient descent&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Now, how can we find the solution $\mathbf{w}^{*}$ to \eqref{eq:opt}? One
simple way is the &lt;strong&gt;gradient descent&lt;/strong&gt;. The method of gradient descent
is to find a &lt;strong&gt;local minimum&lt;/strong&gt; of a cost function $J(\mathbf{w})$,
taking steps in the opposite direction to the gradient
$\nabla{J(\mathbf{w})}$, where&lt;/p&gt;

&lt;p&gt;$$
\nabla{J(\mathbf{w})})=
\begin{bmatrix}
\frac{\partial}{\partial w_0} J(\mathbf{w}) \\&lt;br /&gt;
\frac{\partial}{\partial w_1} J(\mathbf{w}) \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
\frac{\partial}{\partial w_n} J(\mathbf{w})
\end{bmatrix}.
$$&lt;/p&gt;

&lt;p&gt;Consider the Taylor series of
$J(\mathbf{w}-\alpha \nabla{J(\mathbf{w})})$ as a function of $\alpha$:
$$J(\mathbf{w}-\alpha \nabla{J(\mathbf{w})})=J(\mathbf{w})-\alpha\Vert \nabla{J(\mathbf{w})}\Vert^2 + o(\alpha).$$
If $\nabla{J(\mathbf{w})} \ne 0$, then for sufficiently small
$\alpha&amp;gt;0$, we have
$$J(\mathbf{w}-\alpha \nabla{J(\mathbf{w})})&amp;lt;J(\mathbf{w}).$$ This means
that the parameter $\mathbf{w}-\alpha \nabla{J(\mathbf{w})}$ leads to
the smaller value of the cost function than the parameter $\mathbf{w}$.
Intuitively, the gradient $\nabla{J(\mathbf{w})}$ is the direction of
the maximum rate of increase at the parameter $\mathbf{w}$, so moving in
the opposite direction lowers the value of the cost function.&lt;/p&gt;

&lt;p&gt;Thus, we can start with a random guess $\mathbf{w}^{(0)}$ and keep
moving to $\mathbf{w}^{(1)}$, $\mathbf{w}^{(2)}$, $\ldots$,
$\mathbf{w}^{(t)}$ in such a way that
$$
\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)}-\alpha \nabla{J(\mathbf{w}^{(t)})},
\tag{opt:2}\label{eq:gd}
$$
or in other form
$$
w_{j}^{(t+1)} = w_{j}^{(t)}-\alpha \left. \frac{\partial J(\mathbf{w})}{\partial w_{j}} \right\vert_{\mathbf{w}=\mathbf{w}^{(t)}}\text{ for all } j.
\label{eq:gd_element}
$$
Then, for sufficiently small $\alpha&amp;gt;0$, we have, for every $t$,
$$J(\mathbf{w}^{(t+1)}) &amp;lt; J(\mathbf{w}^{(t)}),$$ and $\mathbf{w}^{(t)}$
converges to a local minimum as $t$ grows. When the cost function
$J(\mathbf{w})$ is convex, the local minimum is also the global minimum,
so in such a case, $\mathbf{w}^{(t)}$ can converge to the solution
$\mathbf{w}^{*}$. For non-convex cost functions, the gradient descent
does not guarantee us finding the global minimum. Instead, depending on
the initial guess, we will end up with different local minimum, which is
exemplified in Figure 1.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/gd_initial.png&#34; alt=&#34;The red and green dots represent the trajectories of the gradient descent corresponding to the initial values $w^{(0)}=-135$ and $w^{(0)}=30$, respectively.&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;The effect of the initial guess when $J(w)=(w&amp;#43;90)(w-50)(w/2&amp;#43;50)(w-90)/1000000$.&lt;/h4&gt;
        &lt;p&gt;
        The red and green dots represent the trajectories of the gradient descent corresponding to the initial values $w^{(0)}=-135$ and $w^{(0)}=30$, respectively.
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;h2 id=&#34;learning-rate&#34;&gt;&lt;strong&gt;Learning rate&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In \eqref{eq:gd}, the value of $\alpha$ is called the &lt;strong&gt;learning rate&lt;/strong&gt;. This value determines how fast or slow the parameter moves
towards the optimal solution. Figure 2 illustrates
the effect of the learning rate.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/gd_learning_rate.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;The effect of the learning rate when $J(w)=\frac{1}{2}w^2$ and $w^{(0)}=-20$.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;If $\alpha$ is too small, the gradient descent may take too long to converge. In contrast, the gradient descent
may fail to converge with too large value of $\alpha$.&lt;/p&gt;

&lt;p&gt;Choosing the proper value of the learning rate is not that trivial. In
practice, we often use simply a small enough constant by keeping
decreasing the value until the parameter seems to converge to a certain
point. Or in order to get more accurate solution, one can halve the
value of the learning rate as convergence slows down over iterations.
Another method is to adaptively change the learning rate at every
iteration $t$ in the way that the difference
$ J(\mathbf{w}^{(t)})-J(\mathbf{w}^{(t+1)})$ is maximized. For more
detail on this method, refer to the &lt;strong&gt;steepest descent&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;stochastic-gradient-descent&#34;&gt;&lt;strong&gt;Stochastic gradient descent&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;From \eqref{eq:opt}, the gradient $\nabla{J(\mathbf{w})}$ is represented
as
$$
\nabla{J(\mathbf{w})}=\frac{1}{M} \sum_{m=1}^{M} \nabla{J_m(\mathbf{w})}.
\label{eq:gradient}
$$
 This implies that computing $\nabla{J(\mathbf{w})}$ is equivalent to taking the average of
$\nabla{J_m(\mathbf{w})}$, the gradient of the cost specific to the
$m$-th observation, over the full training set. However, in practice,
the training set is often very large, and thus averaging over the entire
set can take a significant time. For this reason, the &lt;strong&gt;stochastic
gradient descent&lt;/strong&gt; simply approximates the true gradient by a
gradient at a single observation as follows:
$$\nabla{J(\mathbf{w})}\approxeq\nabla{J_m(\mathbf{w})}.
\label{eq:gradient_approx}$$ Thus, with the stochastic gradient descent,
the parameter update equation in \eqref{eq:gd} can be rewritten as
$$
\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)}-\alpha \nabla{J_t(\mathbf{w}^{(t)})}.
\label{eq:sto_gd}
$$&lt;/p&gt;

&lt;p&gt;As a compromise between the true gradient and the gradient at a single
observation, one may also consider the gradient averaged over a few
training samples.&lt;/p&gt;

&lt;h3 id=&#34;practice .unnumbered&#34;&gt;&lt;strong&gt;Practice&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Tensorflow provides a function that implements the gradient descent
algorithm. Thus, you can just use it without deriving an actual
gradient. What you need to do is to choose the value of the learning
rate. The following shows an example.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import tensorflow as tf
import numpy as np

w = tf.Variable(100.0)  # initial guess = 100.0
J = tf.pow(w, 2)        # J(w) = w^2

optimizer = tf.train.GradientDescentOptimizer(0.05)     # learning rate = 0.05
train = optimizer.minimize(J)

# Initialize variables
init = tf.initialize_all_variables()

sess = tf.InteractiveSession()
sess.run(init)

for step in range(0, 201):
    sess.run(train)
    if step % 20 == 0:
        print &amp;quot;%3d, %10.5f, %10.5f&amp;quot; % (step, w.eval(), J.eval())

sess.close()        
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output of the script would be as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  0,   90.00000, 8100.00000
 20,   10.94190,  119.72514
 40,    1.33028,    1.76964
 60,    0.16173,    0.02616
 80,    0.01966,    0.00039
100,    0.00239,    0.00001
120,    0.00029,    0.00000
140,    0.00004,    0.00000
160,    0.00000,    0.00000
180,    0.00000,    0.00000
200,    0.00000,    0.00000
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
