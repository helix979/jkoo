<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ml on Jinkyu Koo</title>
    <link>https://helix979.github.io/jkoo/tags/ml/</link>
    <description>Recent content in Ml on Jinkyu Koo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jinkyu Koo</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/jkoo/tags/ml/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Artificial Neural Networks</title>
      <link>https://helix979.github.io/jkoo/post/ml-ann/</link>
      <pubDate>Tue, 16 Feb 2016 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-ann/</guid>
      <description>

&lt;p&gt;An artificial neural network (ANN) is a machine learning model inspired
by biological neural networks in a human brain. ANNs offer an elegant
way to formulate a complex non-linear hypothesis, due to their
hierarchical virtue of layering non-linear units. Even though possible,
using an ANN for regression is considered overkill in many cases. Thus,
our discussion will be focused on the use for classification.&lt;/p&gt;

&lt;h3 id=&#34;perceptrons&#34;&gt;Perceptrons&lt;/h3&gt;

&lt;p&gt;A neuron, illustrated in Figure 1, is a core
component of a human brain that transmits electrical signals from one
place to others.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/neuron.jpg&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Structure of a biological neuron.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;ANN’s neuron, which we call a perceptron, is limited imitation of the
real neuron. A perceptron has multiple input channels like dendrites and
a single output channel like an axon, as shown in Figure 2.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/perceptron.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;A perceptron.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;For given inputs ${ x_{1}, x_{2}, \ldots, x_{n} }$, a perceptron first makes a weighted
input $z$, defined as $$\begin{align}
z=\sum_{i=0}^{n} w_i x_i,\nonumber\end{align}$$ in which $w_i$ is a weight for
an input $x_i$. Here, $x_0$ is called a bias input that is always set to
1, and normally omitted from the illustration of a perceptron. Then, the
weighted input $z$ is processed by an activation function $a(z)$ to
yield the output. There are many choices to define the activation
function. A few examples are listed below:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Sigmoid (or also known as logistic) $$\begin{align}
a(z) = \frac{1}{1 + e^{-z}}
\tag{ann:1}\label{eq:act_sigmoid}\end{align}$$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Rectified linear unit (often called Relu in short) $$\begin{align}
a(z) = \max(0,z)\nonumber\end{align}$$&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Hyperbolic tangent $$\begin{align}
a(z)=\tanh(z)\nonumber\end{align}$$&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As Figure 3 shows, a family of activation
functions regulates the output to stay at zero until the weighted input
$z$ is larger than a certain threshold at which the output quickly
increases.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/activation.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Plots of activation functions.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;h3 id=&#34;structure-of-anns&#34;&gt;Structure of ANNs&lt;/h3&gt;

&lt;p&gt;An ANN is an interconnected group of perceptrons, as exemplified in
Figure 4.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/nn.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;An artificial neural network.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;As indicated, outputs of the perceptrons in a layer are fed into the
next layer as inputs until the final layer, called an output layer,
produces outputs. Here, the first layer is called an input layer where
we enter sample observations ${ x_{1}, x_{2}, \ldots, x_{n} }$. The
layers between the input layer and output layer are called hidden
layers. Adding more hidden layers allows us to model a more complex
non-linear function, since the final outputs are composition of multiple
activation functions, which are non-linear.&lt;/p&gt;

&lt;p&gt;Let $a_i^{[l]}$ be the output of the $i$-th perceptron at layer $l$ with
$a_0^{[l]}=1$ (bias units) and $a_i^{[1]}=x_i$ (inputs). Now, we
represent the weighted input to the $i$-th perceptron at layer $l$ as
$$\begin{align}
z_i^{[l]}=\sum_{j=0}^{n^{[l-1]}}w_{ij}^{[l-1]} a_j^{[l-1]},\nonumber\end{align}$$
where $n^{[l-1]}$ is the number of perceptrons at layer $(l-1)$, and
$w_{ij}^{[l-1]}$ denotes the weight from the $j$-th perceptron at layer
$(l-1)$ to the $i$-th perceptron at layer $l$. Then, we can write
$a_i^{[l]}$ as $$\begin{align}
a_i^{[l]} = a(z_i^{[l]}).\nonumber\end{align}$$&lt;/p&gt;

&lt;h3 id=&#34;multiclass-classification&#34;&gt;Multiclass classification&lt;/h3&gt;

&lt;p&gt;Given an observation pair of an input vector
$\mathbf{x}_{m} = \begin{bmatrix} x_{m,0} &amp;amp; x_{m,1} &amp;amp; \cdots &amp;amp; x_{m,n} \end{bmatrix}^{T} \in \mathbb{R}^{n+1}$
and an output $y_{m} \in {1,2,\ldots,K }$ for $m=1,2,\ldots,M$,
suppose that we want to make a multiclass classifier using an ANN and
the one-versus-rest strategy. Then, since there $K$ classes to classify,
the output layer needs to have $K$ perceptrons, one of which results in
a high value when a corresponding data comes in. The hypothesis function
is simply the vector that represents the output layer. That is, assuming
that there are $L$ layers, the hypothesis function
$\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m})$ is given as $$\begin{align}
\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m})=
\begin{bmatrix}
a_{m,1}^{[L]} \\&lt;br /&gt;
a_{m,2}^{[L]} \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
a_{m,K}^{[L]} \\&lt;br /&gt;
\end{bmatrix},\nonumber\end{align}$$ where $\mathbf{W}$ is a set of all weights
${ w_{ij}^{[l]} }$, and $a_{m,i}^{[l]}$ denotes $a_{i}^{[l]}$ when
$\mathbf{x}_{m}$ is entered into an ANN. For example, if the activation
functions are defined by the sigmoid in \eqref{eq:act_sigmoid}, what we
need to do for training is to choose $w_{ij}^{[l]}$ for all $i,j,l$ so
that $\mathbf{h}(\mathbf{x}_{m})$ gets close to $\mathbf{e}_k$ given in
&lt;a href=&#34;https://helix979.github.io/jkoo/jkoo/post/ml-logistic/#mjx-eqn-eqstd_basis&#34; target=&#34;_blank&#34;&gt;(logi:4)&lt;/a&gt; when $y_m=k$. Similarly to &lt;a href=&#34;https://helix979.github.io/jkoo/jkoo/post/ml-logistic/#mjx-eqn-eqlogistic_cost_multi&#34; target=&#34;_blank&#34;&gt;(logi:5)&lt;/a&gt;, the cost function $J(\mathbf{W})$ is
then defined as $$\begin{align}
J(\mathbf{W})
= \frac{1}{M}\sum_{m=1}^{M} J_m(\mathbf{W}),
\label{eq:nn_cost_multi}\nonumber\end{align}$$ where $$\begin{align}
J_m(\mathbf{W})
= -\sum_{k=1}^{K}
\left( \mathbf{1}_{m}(k) \log(a_{m,k}^{[L]}) +  ( 1-\mathbf{1}_{m}(k)) \log( 1 - a_{m,k}^{[L]} ) \right).
\label{eq:nn_cost_multi2}\nonumber\end{align}$$&lt;/p&gt;

&lt;h3 id=&#34;backpropagation&#34;&gt;Backpropagation&lt;/h3&gt;

&lt;p&gt;In order to have the optimal $\mathbf{W}$, we need to minimize the cost
function $J(\mathbf{W})$, which can be done by using the gradient
descent. In this case, the gradient descent equations are&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
w_{ij}^{[l](t+1)} = w_{ij}^{[l](t)} - \alpha \left. \frac{\partial J(\mathbf{W})}{\partial w_{ij}^{[l]}} \right \vert_{\mathbf{W}=\mathbf{W}^{(t)}}
\tag{ann:2}\label{eq:ann_gd}.
\end{align}
$$&lt;/p&gt;

&lt;p&gt;However, it is not easy to derive a
closed-form of $\partial J(\mathbf{W}) / \partial w_{ij}^{[l]}$, since
$J(\mathbf{W})$ is a composite function of sigmoids. For this reason,
training an ANN is conducted in conjunction with backpropagation.&lt;/p&gt;

&lt;p&gt;The backpropagation, an abbreviation for “backward propagation of
errors&amp;rdquo;, provides an alternative way to compute
$\partial J(\mathbf{W}) / \partial w_{ij}^{[l]}$. To see this, we first
define an error $\delta_{m,i}^{[l]}$ of the $i$-th perceptron at layer
$l$ according to $\mathbf{x}_{m}$ as $$\begin{align}
\delta_{m,i}^{[l]} = \frac{\partial J_m(\mathbf{W})}{\partial z_{m,i}^{[l]}},
\tag{ann:3}\label{eq:ann_bp0}\end{align}$$ in which $z_{m,i}^{[l]}$ denotes
$z_{i}^{[l]}$ when $\mathbf{x}_{m}$ comes into the network. Then, we
first note that $$\begin{align}
\frac{\partial J(\mathbf{W})}{\partial w_{ij}^{[l]}}
&amp;amp;= \frac{1}{M}\sum_{m=1}^{M} \frac{\partial J_m(\mathbf{W})}{\partial w_{ij}^{[l]}} \nonumber\\&lt;br /&gt;
&amp;amp;= \frac{1}{M}\sum_{m=1}^{M} \frac{\partial z_{m,i}^{[l+1]}}{\partial w_{ij}^{[l]}}\frac{\partial J_m(\mathbf{W})}{\partial z_{m,i}^{[l+1]}} \nonumber\\&lt;br /&gt;
&amp;amp;=\frac{1}{M}\sum_{m=1}^{M}a_{m,j}^{[l]} \delta_{m,i}^{[l+1]}. \tag{ann:4}\label{eq:ann_bp1}\end{align}$$
This means that since $a_{m,j}^{[l]}$ is a known value (computed through
the ANN), $\partial J(\mathbf{W}) / \partial w_{ij}^{[l]}$ can be
calculated by obtaining $\delta_{m,i}^{[l+1]}$.&lt;/p&gt;

&lt;p&gt;In the meantime, using a chain rule, we have $$\begin{align}
\delta_{m,i}^{[l]} &amp;amp; = \frac{\partial J(\mathbf{W})}{\partial z_{m,i}^{[l]}}  \nonumber\\&lt;br /&gt;
&amp;amp; = \sum_k \frac{\partial J(\mathbf{W})}{\partial z_{m,k}^{[l+1]}} \frac{\partial z_{m,k}^{[l+1]}}{\partial z_{m,i}^{[l]}} \nonumber\\\
&amp;amp; = \sum_k \delta_{m,k}^{[l+1]} \frac{\partial z_{m,k}^{[l+1]}}{\partial z_{m,i}^{[l]}}.\nonumber\end{align}$$
Since
$z_{m,k}^{[l+1]} = \sum_r w_{kr}^{[l]} a_{m,r}^{[l]} = \sum_r w_{kr}^{[l]} a(z_{m,r}^{[l]})$,
we know $$\begin{align}
\frac{\partial z_{m,k}^{[l+1]}}{\partial z_{m,i}^{[l]}} = w_{ki}^{[l]} a&amp;rsquo;(z_{m,i}^{[l]}).\nonumber\end{align}$$
Therefore, we can represent $\delta_{m,i}^{[l]}$ as $$\begin{align}
\delta_{m,i}^{[l]} = a&amp;rsquo;(z_{m,i}^{[l]}) \sum_k \delta_{m,k}^{[l+1]} w_{ki}^{[l]}.
\tag{ann:5}\label{eq:ann_bp2}\end{align}$$ Notice in \eqref{eq:ann_bp2} that
$a&amp;rsquo;(z_{m,i}^{[l]})$ and $w_{ki}^{[l]}$ are all known. Thus, we see that
$\delta_{m,i}^{[l]}$ is determined by $ \delta_{m,k}^{[l+1]}$ for all
$k$. In other words, once we know $\delta_{m,i}^{[L]}$ for all $i$, then
we can compute $\delta_{m,i}^{[L-1]}$, $\delta_{m,i}^{[L-2]}$, $\ldots$,
$\delta_{m,i}^{[2]}$ for all $i$ iteratively using \eqref{eq:ann_bp2}.&lt;/p&gt;

&lt;p&gt;Fortunately, we can easily compute $\delta_{m,i}^{[L]}$, expressing it
in terms of the output activation, $$\begin{align}
\delta_{m,i}^{[L]} &amp;amp;= \frac{\partial J_m(\mathbf{W})}{\partial a_{m,i}^{[L]}} \frac{\partial a_{m,i}^{[L]}}{\partial z_{m,i}^{[L]}} \nonumber\\&lt;br /&gt;
&amp;amp;= \frac{\partial J_m(\mathbf{W})}{\partial a_{m,i}^{[L]}} a&amp;rsquo;(z_{m,i}^{[L]}). \tag{ann:6}\label{eq:ann_bp3}\end{align}$$
Here, $a&amp;rsquo;(z_{m,i}^{[L]})$ is known, and
$\partial J_m(\mathbf{W})/\partial a_{m,i}^{[L]}$ can also be calculated
without difficult, since $J_m(\mathbf{W})$ is a function of
$a_{m,i}^{[L]}$.&lt;/p&gt;

&lt;p&gt;In summary, backpropagation represented by four equations
\eqref{eq:ann_bp0}, \eqref{eq:ann_bp1} \eqref{eq:ann_bp2}, and
\eqref{eq:ann_bp3}, is an algorithm to compute $\delta_{m,i}^{[l]}$
iteratively from $l=L$ to $l=2$, by which we can eventually obtain
$\partial J(\mathbf{W}) / \partial w_{ij}^{[l]}$. Now, the gradient
descent with backpropagation can be stated as follows.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Choose the initial value of $w_{ij}^{[l]}$ randomly.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Do the following for $m=1,2,\ldots,M$.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Set $a_{m,i}^{[1]} = x_{m,i}$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Perform forward propagation to compute $a_{m,i}^{[l]}$ for
$l = 2,3,\dots ,L$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Perform backpropagation to compute $\delta_{m,i}^{[l]}$ for
$l = L, L-1,\dots ,2$.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Set
$\frac{\partial J(\mathbf{W})}{\partial w_{ij}^{[l]}}=\frac{1}{M}\sum_{m=1}^{M}a_{m,j}^{[l]} \delta_{m,i}^{[l+1]}$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Update $w_{ij}^{[l]}$ using \eqref{eq:ann_gd}.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Repeat steps 2 to 4 until convergence.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;practice&#34;&gt;Practice&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import random
import sys


n_data = 10000
r_x = 10
n_class = 3

#---------------------------------------------------------------------
def create_data(n):
    &amp;quot;&amp;quot;&amp;quot;
    This function will make a set of data such that
    a random number between c*r_x and (c+1)*r_x is given a label c. 
    &amp;quot;&amp;quot;&amp;quot;
    dataset = []
    for i in range(n):
        c = np.random.randint(n_class)
        x_1 = np.random.rand() * r_x + c*r_x
        y = c
        sample = [x_1, y]
        dataset.append(sample)
    random.shuffle(dataset)
    point_, label_ = zip(*dataset)
    _point_ = np.float32(np.array([point_]))
    _label_ = np.zeros([n_class, n])
    for i in range(len(label_)):
        _label_[label_[i]][i] = 1
    return _point_, _label_
#---------------------------------------------------------------------

# Create a dataset for training
point, label = create_data(n_data)

# Placeholders to take data in
x = tf.placeholder(tf.float32, [1, None])
y = tf.placeholder(tf.float32, [n_class, None])

# Write a model
w1 = tf.Variable(tf.random_uniform([4, 1], -1.0, 1.0))
w1_0 = tf.Variable(tf.random_uniform([4, 1], -1.0, 1.0))
layer2 = tf.sigmoid(tf.matmul(w1, x) + w1_0)

w2 = tf.Variable(tf.random_uniform([n_class, 4], -1.0, 1.0))
w2_0 = tf.Variable(tf.random_uniform([n_class, 1], -1.0, 1.0))
layer3 = tf.sigmoid(tf.matmul(w2, layer2) + w2_0)

cost = -tf.reduce_sum(y*tf.log(layer3)+(1-y)*tf.log(1-layer3))/n_data
optimizer = tf.train.GradientDescentOptimizer(0.001)
train = optimizer.minimize(cost)

# Compute accuracy
label_hat_ = tf.argmax(layer3,0)
correct_cnt = tf.equal(tf.argmax(y,0), label_hat_)
accuracy = tf.reduce_mean(tf.cast(correct_cnt, &amp;quot;float&amp;quot;))


sess = tf.InteractiveSession()

# Initialize variables
init = tf.initialize_all_variables()
sess.run(init)

# Learning
step = 0
while 1:
    try:
        step += 1
        train.run(feed_dict={x: point, y: label})

        if step % 100 == 0:
            print step
            print w1.eval()
            print w1_0.eval()
            print w2.eval()
            print w2_0.eval()

    # Ctrl+c will stop training
    except KeyboardInterrupt:
        break


# Create another dataset for test
point_t, label_t = create_data(100)
rate = accuracy.eval(feed_dict={x: point_t, y: label_t})
print &amp;quot;\n\n accuracy = %s\n&amp;quot; % (rate)

# Plot the test results
plt.plot(point_t[0,:], label_hat_.eval(feed_dict={x: point_t}), &#39;o&#39;)
plt.grid()
plt.ylim(-1, n_class)

xt = range(0, n_class*10+1, 10)
yt = range(-1, n_class, 1)
plt.step(xt, yt, &#39;r--&#39;)

plt.savefig(&#39;ann_test.pdf&#39;)

sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 5 is what you may get from the code above.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/ann_test.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Blue dots that are not on the red line indicate classification errors.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Support Vector Machines</title>
      <link>https://helix979.github.io/jkoo/post/ml-svm/</link>
      <pubDate>Sun, 14 Feb 2016 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-svm/</guid>
      <description>

&lt;p&gt;Support vector machine (SVM) is a learning model based on the concept of
a separating hyperplane that defines the decision boundary. SVM can be
used for both classification and regression analysis, but here we
explain it focusing on classification.&lt;/p&gt;

&lt;h3 id=&#34;separating-hyperplane-of-the-maximum-margin&#34;&gt;Separating hyperplane of the maximum margin&lt;/h3&gt;

&lt;p&gt;Consider a training set shown in Figure 1, where data
samples belong to one of two classes. The separating hyperplane is the
decision boundary that divides the two classes by a hyperplane (a line
in two dimensions, a plane in three dimensions, and so on). The solid
line and the dashed line in the figure are examples of the separating
hyperplanes that we can set in this problem. Between the two choices, we
may prefer the solid line to the dashed line, because the solid line
separates the classes with larger geometric margin and thus it will
generalize better to unseen data samples. SVM provides an analytical way
to find out the separating hyperplane that has the largest margin. For
this reason, a SVM classifier is also called a maximum-margin
classifier.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/svm_plane.png&#34; alt=&#34;Although both the solid and dashed lines can separate the squares from dots, the solid one would be preferred.&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Separating hyperplanes.&lt;/h4&gt;
        &lt;p&gt;
        Although both the solid and dashed lines can separate the squares from dots, the solid one would be preferred.
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;linearly-separable-classes&#34;&gt;Linearly separable classes&lt;/h2&gt;

&lt;p&gt;Say we are given a training set of $M$ points where the $m$-th input
vector is denoted by $\mathbf{x}_{m} \in \mathbb{R}^{n}$ and its
corresponding label by $y_{m} \in {-1,1}$. The separating hyperplane
can be represented as the set of points $\mathbf{x} \in \mathbb{R}^{n}$
that satisfies the following equation: $$\begin{align}
\mathbf{w}^T\mathbf{x}+b=0,\nonumber\end{align}$$ where
$\mathbf{w} \in \mathbb{R}^{n}$ and $b \in \mathbb{R}$.&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; Note that
$\mathbf{w}$ is normal to the hyperplane. If the training samples are
linearly separable, we can select two hyperplanes that are parallel to
the separating hyperplane and separate the classes without leaving any
data samples between them. Without loss of generality, these hyperplanes
can be written as&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
\mathbf{w}^T\mathbf{x}+b=1
\end{align}\tag{svm:1}\label{eq:hyperplane1}
$$&lt;/p&gt;

&lt;p&gt;and
$$
\begin{align}
\mathbf{w}^T\mathbf{x}+b=-1.
\end{align}\tag{svm:2}\label{eq:hyperplane2}
$$&lt;/p&gt;

&lt;p&gt;The training samples lying on such
hyperplanes are called &lt;strong&gt;support vectors&lt;/strong&gt;. Figure 2 describes the support vectors.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/svm_sv.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Support vectors.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;Now, consider two support vectors $\mathbf{x}_i$ and $\mathbf{x}_j$ such
that&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
\mathbf{w}^T\mathbf{x}_i+b=1
\end{align}\tag{svm:3}\label{eq:sv1}
$$&lt;/p&gt;

&lt;p&gt;and
$$
\begin{align}
\mathbf{w}^T\mathbf{x}_j+b=-1,
\end{align}\tag{svm:4}\label{eq:sv2}
$$&lt;/p&gt;

&lt;p&gt;that is, $\mathbf{x}_i$ and $\mathbf{x}_j$
are support vectors that do not belong to the same class. Subtracting
\eqref{eq:sv2} from \eqref{eq:sv1} and rescaling with $1/\Vert w \Vert$, we
get:&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
\frac{\mathbf{w}^T}{\Vert w \Vert}(\mathbf{x}_i-\mathbf{x}_j)=\frac{2}{\Vert w \Vert}.
\end{align}\tag{svm:5}\label{eq:margin}
$$&lt;/p&gt;

&lt;p&gt;Note that the projection of a vector
$\mathbf{x}_i-\mathbf{x}_j$ onto $\mathbf{w}/\Vert w \Vert$ measures the
margin, the distance between the two hyperplanes defined in
\eqref{eq:hyperplane1} and \eqref{eq:hyperplane2}. From \eqref{eq:margin}, the
margin is equal to $2/\Vert w \Vert$. Therefore, to maximize the margin,
we have to minimize $\Vert w \Vert$, which leads to the following
optimization problem:&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
&amp;amp;\min_{\mathbf{w},b} \frac{\Vert w \Vert^2}{2} \tag{svm:6}\label{eq:svm_opt1} \\&lt;br /&gt;
\text{subject to } &amp;amp; y_m (\mathbf{w}^T \mathbf{x}_m+b) \geq 1 \text{ for all } m.  \tag{svm:7}\label{eq:svm_opt2}
\end{align}
$$&lt;/p&gt;

&lt;h3 id=&#34;learning&#34;&gt;Learning&lt;/h3&gt;

&lt;p&gt;Now introducing Lagrange multipliers
$\mathbf{\alpha} = \begin{bmatrix} \alpha_1 &amp;amp; \alpha_2 &amp;amp; \cdots &amp;amp; \alpha_M \end{bmatrix}^{T} \in \mathbb{R}^{M}$
for the constraints in \eqref{eq:svm_opt2}, we form the Lagrangian
function $L(\mathbf{w},b,\mathbf{\alpha})$ as $$\begin{align}
L(\mathbf{w},b,\mathbf{\alpha})=\frac{\Vert w \Vert^2}{2}-\sum_{m=1}^{M} \alpha_m (y_m (\mathbf{w}^T \mathbf{x}_m+b) -1).
\tag{svm:8}\label{eq:svm_Lagrangian}\end{align}$$ Then, by Wolfe duality, the
solution to \eqref{eq:svm_opt1} can be obtained by solving the following
dual problem:&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
&amp;amp; \max_{\mathbf{w},b,\mathbf{\alpha}} L(\mathbf{w},b,\mathbf{\alpha}) \tag{svm:9}\label{eq:svm_opt3}\\&lt;br /&gt;
\text{subject to } &amp;amp; \frac{\partial}{\partial w_j}L(\mathbf{w},b,\mathbf{\alpha})=0 \text{ for all } m, \tag{svm:10}\label{eq:svm_opt4}\\&lt;br /&gt;
&amp;amp; \frac{\partial}{\partial b}L(\mathbf{w},b,\mathbf{\alpha})=0, \tag{svm:11}\label{eq:svm_opt5}\\&lt;br /&gt;
&amp;amp; \alpha_m \ge 0 \text{ for all } m \nonumber\label{eq:svm_opt6}
\end{align}
$$&lt;/p&gt;

&lt;p&gt;From \eqref{eq:svm_opt4} and \eqref{eq:svm_opt5}, we get $$\begin{align}
\mathbf{w}=\sum_{m=1}^{M} \alpha_m y_m \mathbf{x}_m
\tag{svm:12}\label{eq:svm_opt7}\end{align}$$ and $$\begin{align}
\sum_{m=1}^{M} \alpha_m y_m =0.
\tag{svm:13}\label{eq:svm_opt8}\end{align}$$ Substituting \eqref{eq:svm_opt7} and
\eqref{eq:svm_opt8} into \eqref{eq:svm_Lagrangian}, we can rewrite the
optimization problem in \eqref{eq:svm_opt3} as follows:&lt;/p&gt;

&lt;p&gt;$$
\begin{align}
&amp;amp; \max_{\mathbf{\alpha}} \left( \sum_{m=1}^M \alpha_m - \frac{1}{2}\sum_{i=1}^M \sum_{j=1}^M \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j \right) \tag{svm:14}\label{eq:svm_opt11}\\&lt;br /&gt;
&amp;amp; \text{subject to } \sum_{m=1}^{M} \alpha_m y_m =0, \label{eq:svm_opt13} \nonumber \\&lt;br /&gt;
&amp;amp; \alpha_m \ge 0 \text{ for all } m \label{eq:svm_opt14} \nonumber
\end{align}
$$&lt;/p&gt;

&lt;p&gt;This is a quadratic programming problem with respect to
$\mathbf{\alpha}$, and can be solved by a variety of methods
(&lt;em&gt;e.g.&lt;/em&gt;, the conjugate gradient). Having obtained the optimal
$\mathbf{\alpha}$, we can determine the optimal $\mathbf{w}$ from
\eqref{eq:svm_opt7}.&lt;/p&gt;

&lt;p&gt;The optimal $b$ still remains unknown. Now we consider the
Karush-Kuhn-Tucker (KKT) complementary slackness conditions, which are
given by $$\begin{align}
\alpha_m (y_m (\mathbf{w}^T \mathbf{x}_m+b) -1)=0  \text{ for all } m.\nonumber\end{align}$$
From the conditions above, we learn:&lt;/p&gt;

&lt;p&gt;$$
\alpha_m = \left\{
\begin{matrix}
0 &amp;amp; \text{if } y_m (\mathbf{w}^T \mathbf{x}_m+b) -1 &amp;gt; 0,\\&lt;br /&gt;
\text{non-negative} &amp;amp; \text{if } y_m (\mathbf{w}^T \mathbf{x}_m+b) -1 = 0.\\&lt;br /&gt;
\end{matrix} \right.
$$&lt;/p&gt;

&lt;p&gt;This means that for non-zero $\alpha_m$ obtained
in \eqref{eq:svm_opt11}, the corresponding $\mathbf{x}_m$ are support
vectors that satisfy $y_m (\mathbf{w}^T \mathbf{x}_m+b) -1 = 0$. Thus,
given a support vector $\mathbf{x}_m$, the optimal value of $b$ can be
obtained as $$\begin{align}
b=\frac{1}{y_m}-\mathbf{w}^T \mathbf{x}_m.
\tag{svm:15}\label{eq:svm_b}\end{align}$$ In practice, we can have a more robust
estimate of $b$ by taking an average over various support vectors
instead of a single one.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;soft-margin-linearly-non-separable-classes&#34;&gt;Soft margin: linearly non-separable classes&lt;/h2&gt;

&lt;p&gt;If the training set is not linearly separable, we will find no feasible
solution to \eqref{eq:svm_opt1}. To deal with such a case, we introduce a
modification called &lt;strong&gt;soft margin&lt;/strong&gt;. The soft margin method relaxes
the constraints in \eqref{eq:svm_opt2} with non-negative slack variables
$\xi_m$ as follows:&lt;/p&gt;

&lt;p&gt;$$\begin{align}
y_m (\mathbf{w}^T \mathbf{x}_m+b) \ge 1- \xi_m  \text{ for all } m.\nonumber\end{align}$$&lt;/p&gt;

&lt;p&gt;Note that every constraint can be satisfied if $\xi_m$ is sufficiently
large. What we actually want is to choose the value of $\xi_m$ to be
non-zero only when necessary and as much as necessary. Such a case is
when $\mathbf{x}_m$ is an outlier violating the constraint in
\eqref{eq:svm_opt2}. The separating hyperplane is then regarded as the
solution to the following optimization problem:&lt;/p&gt;

&lt;p&gt;$$\small
\begin{align}
&amp;amp;\min_{\mathbf{w},b, \xi_1,\ldots,\xi_M} \left( \frac{\Vert w \Vert^2}{2} +C\sum_{m=1}^M \xi_m \right) \tag{svm:16}\label{eq:svm_soft1} \\&lt;br /&gt;
 \text{subject to } &amp;amp; y_m (\mathbf{w}^T \mathbf{x}_m+b) \geq 1 -\xi_m  \text{ and } \xi_m \ge 0 \text{ for all } m,  \tag{svm:17}\label{eq:svm_soft2}
\end{align}$$&lt;/p&gt;

&lt;p&gt;where $C$ is a parameter that controls the solution’s sensitivity to
outliers. When $C$ is small, the outliers are almost ignored and large
margin is achieved. In contrast, a large $C$ allows a small number of
outliers to exist at the cost of narrow margin.&lt;/p&gt;

&lt;h3 id=&#34;learning-1&#34;&gt;Learning&lt;/h3&gt;

&lt;p&gt;Define the Lagrangian function
$L(\mathbf{w},b,\mathbf{\alpha},\mathbf{\beta})$ as $$\small\begin{align}
L(\mathbf{w},b,\mathbf{\alpha},\mathbf{\beta})=\frac{\Vert w \Vert^2}{2}-\sum_{m=1}^{M} \alpha_m (y_m (\mathbf{w}^T \mathbf{x}_m+b) -1+\xi_m)-\sum_{m=1}^{M} \beta_m \xi_m,
\label{eq:svm_soft_Lagrangian}\nonumber\end{align}$$ in which
$\mathbf{\alpha} = \begin{bmatrix} \alpha_1 &amp;amp; \alpha_2 &amp;amp; \cdots &amp;amp; \alpha_M \end{bmatrix}^{T} \in \mathbb{R}^{M}$
and
$\mathbf{\beta} = \begin{bmatrix} \beta_1 &amp;amp; \beta_2 &amp;amp; \cdots &amp;amp; \beta_M \end{bmatrix}^{T} \in \mathbb{R}^{M}$
are Lagrange multipliers. Then, we can have the solution to
\eqref{eq:svm_soft1} by solving its Wolfe dual problem given as&lt;/p&gt;

&lt;p&gt;$$\begin{align}
&amp;amp; \max_{\mathbf{w},b,\mathbf{\alpha},\mathbf{\beta}} L(\mathbf{w},b,\mathbf{\alpha},\mathbf{\beta}) \tag{svm:18}\label{eq:svm_soft3}\\&lt;br /&gt;
\text{subject to } &amp;amp; \frac{\partial}{\partial w_j}L(\mathbf{w},b,\mathbf{\alpha},\mathbf{\beta})=0 \text{ for all } m, \tag{svm:19}\label{eq:svm_soft4}\\&lt;br /&gt;
&amp;amp; \frac{\partial}{\partial b}L(\mathbf{w},b,\mathbf{\alpha},\mathbf{\beta})=0, \tag{svm:20}\label{eq:svm_soft5}\\&lt;br /&gt;
&amp;amp; \frac{\partial}{\partial \xi_m}L(\mathbf{w},b,\mathbf{\alpha},\mathbf{\beta})=0  \text{ for all } m, \tag{svm:21}\label{eq:svm_soft6}\\&lt;br /&gt;
&amp;amp; \alpha_m \ge 0 \text{ and } \beta_m \ge 0 \text{ for all } m. \tag{svm:22}\label{eq:svm_soft7}\end{align}$$&lt;/p&gt;

&lt;p&gt;From \eqref{eq:svm_soft6} and \eqref{eq:svm_soft7}, we have
$$\begin{align}
\beta_m = C - \alpha_m \ge 0 \text{ for all } m \nonumber\end{align}$$ and thus
$$\begin{align}
\alpha_m \le C \text{ for all } m.\nonumber\end{align}$$ By substituting
\eqref{eq:svm_soft4}, \eqref{eq:svm_soft5}, and \eqref{eq:svm_soft6} into
\eqref{eq:svm_soft3}, one can show that $\alpha$ is the solution to:
$$\begin{align}
&amp;amp; \max_{\mathbf{\alpha}} \left( \sum_{m=1}^M \alpha_m - \frac{1}{2}\sum_{i=1}^M \sum_{j=1}^M \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j \right) \nonumber\label{eq:svm_soft11}\\&lt;br /&gt;
&amp;amp;\text{subject to }  \sum_{m=1}^{M} \alpha_m y_m =0, \nonumber\label{eq:svm_soft12}\\&lt;br /&gt;
&amp;amp; 0 \le \alpha_m \le C \text{ for all } m. \nonumber\label{eq:svm_soft13}\end{align}$$
From \eqref{eq:svm_soft4}, the optimal $\mathbf{w}$ is then again
formulated as \eqref{eq:svm_opt7}. To derive the optimal value of $b$, we
can use the KKT complementary slackness conditions as before, which are
given by $$\small\begin{align}
\alpha_m (y_m (\mathbf{w}^T \mathbf{x}_m+b) -1+\xi_m)=0 \text{ and } (C-\alpha_m)\xi_m=0 \text{ for all } m.
\tag{svm:23}\label{eq:svm_kkt}\end{align}$$ Theses conditions imply that if
$0&amp;lt;\alpha_m&amp;lt;C$, we have $\xi_m=0$ and the corresponding $\mathbf{x}_m$
is a support vector. Thus, the optimal $b$ in the soft margin method can
also be estimated as \eqref{eq:svm_b}.&lt;/p&gt;

&lt;h3 id=&#34;hinge-loss&#34;&gt;Hinge loss&lt;/h3&gt;

&lt;p&gt;It is worth noting that we can rewrite \eqref{eq:svm_soft1} and
\eqref{eq:svm_soft2} into a form of unconstrained optimization as
follows: $$\begin{align}
&amp;amp;\min_{\mathbf{w},b} \left( \frac{\Vert w \Vert^2}{2} +C\sum_{m=1}^M \max(0,1-y_m (\mathbf{w}^T \mathbf{x}_m+b)) \right).
\tag{svm:24}\label{eq:svm_hinge1}\end{align}$$ In this form, we see that the SVM
is also a technique of empirical cost minimization with a regularization
term $\Vert w \Vert^2 /2$, where cost function is given by the hinge
loss, &lt;em&gt;i.e.&lt;/em&gt;, $$\begin{align}
\text{loss}(y_m, \hat{y}_m) = \max(0, 1-y_m \hat{y}_m)\nonumber\end{align}$$
with $\hat{y}_m=\mathbf{w}^T \mathbf{x}_m+b$. Since \eqref{eq:svm_hinge1}
is an unconstrained optimization problem, we can solve it using the
gradient descent instead of quadratic programming solvers.&lt;/p&gt;

&lt;h3 id=&#34;practice&#34;&gt;Practice&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

#--------------------------------------------------------
def draw_svm(point, label, w, b, fname):
    for p, y in zip(point, label):
        if (y == -1):
            plt.plot(p[0], p[1], &#39;go&#39;, markersize=10)
        else:
            plt.plot(p[0], p[1], &#39;rs&#39;, markersize=10)

    x = np.arange(1,8,0.1)
    a = -w[0]/w[1]
    y0 = -b/w[1]
    y = a * x + y0
    plt.plot(x, y, &#39;k-&#39;, lw=2)
    plt.savefig(fname)
#--------------------------------------------------------
C = 5.0
_point = [(6.4, 3.5), (7.4, 2.1), (5.0, 3.5), (5.5, 6), (5.9, 2.5),\
         (5, 2), (2.5, -1.9), (4.8, -6.1), (3, -6), (2.3, -0.5), (2.1, -2.3)]
_label = (-1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1)

x = np.float32(np.array(_point))
y = np.float32(np.transpose(np.array(_label, ndmin=2)))

# Write a model
w = tf.Variable(tf.random_uniform([2, 1], -1.0, 1.0))
b = tf.Variable(tf.random_uniform([1], -1.0, 1.0))
y_hat = tf.matmul(x, w) + b


cost = tf.matmul(tf.transpose(w), w)/2 \
        + C * tf.maximum(tf.constant(0.), tf.constant(1.) - y*y_hat)
optimizer = tf.train.GradientDescentOptimizer(0.001)
train = optimizer.minimize(cost)

sess = tf.InteractiveSession()

# Initialize variables
init = tf.initialize_all_variables()
sess.run(init)

# Learning
step = 0
while 1:
    try:
        step += 1
        sess.run(train)

        if step % 100 == 0:
            print step
            print w.eval()
            print b.eval()

    # Ctrl+c will stop training
    except KeyboardInterrupt:
        break


# Plot the results
draw_svm(_point, _label, w.eval(), b.eval(), &#39;svm_hinge.pdf&#39;)

sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 3 shows the result of the code above.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/svm_hinge.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Result of the code above.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;kernel-non-linear-svm&#34;&gt;Kernel: non-linear SVM&lt;/h2&gt;

&lt;p&gt;So far we have tried to find a hyperplane that divides data into two
classes. However, in practice, we are often confronted with the case
where it is by no means reasonable to assume that training set is
linearly separable. Figure 4 illustrates such an
example. In this case, it is clear that a hyperplane is not a good
option to separate classes. Rather, a circle-shape decision boundary
would be a better choice.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/svm_nonlinear1.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;No hyperplane separates data in two dimensions.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;A trick to handle this situation is to map data into a higher
dimensional space, which we call a &lt;strong&gt;feature space&lt;/strong&gt;. Suppose that
we convert the training data using a non-linear mapping $\Phi(\cdot)$
that performs transformation like $$\begin{align}
\Phi(\mathbf{x}) \in \mathbb{R}^{n&amp;rsquo;} \text{ for } \mathbf{x} \in \mathbb{R}^{n},\nonumber\end{align}$$
where $n&amp;rsquo;&amp;gt;n$. Figure 5 shows an example where a
mapping function
$\Phi(\mathbf{x})=\begin{bmatrix} x_1 &amp;amp; x_2 &amp;amp; x_1^2+x_2^2 \end{bmatrix}^T$
transforms data in Figure 4 into a feature space
of three dimensions.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/svm_nonlinear2.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Mapped into three dimensions where there is a hyperplane that separates data.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;The data samples in Figure 5 are linearly separable and thus we can find a
separating hyperplane instead of a non-linear decision boundary. In
general, mapping into a higher-dimension space gives us a possibility to
find a linear decision boundary for the case that cannot do otherwise.
To apply this trick to SVMs, all we need to do is to replace
$\mathbf{x}$ with $\Phi(\mathbf{x})$ in learning equations, &lt;em&gt;e.g.&lt;/em&gt;,
\eqref{eq:svm_opt7}, \eqref{eq:svm_opt11}, and \eqref{eq:svm_b}.&lt;/p&gt;

&lt;p&gt;Computation in the feature space is typically costly because it is high
dimensional. However, SVMs can avoid this issue by adopting another
trick, called &lt;strong&gt;kernel trick&lt;/strong&gt;. Changing $\mathbf{x}$ with
$\Phi(\mathbf{x})$, the optimization objective \eqref{eq:svm_opt11} is
rewritten as $$\begin{align}
\max_{\mathbf{\alpha}} \left( \sum_{m=1}^M \alpha_m - \frac{1}{2}\sum_{i=1}^M \sum_{j=1}^M \alpha_i \alpha_j y_i y_j \Phi(\mathbf{x}_i)^T \Phi(\mathbf{x}_j) \right).
\tag{svm:25}\label{eq:svm_opt11r}\end{align}$$ The separating hyperplane, applying
\eqref{eq:svm_opt7} and \eqref{eq:svm_b}, is also rewritten as
$$\begin{align}
\sum_{i=1}^{M} \alpha_i y_i \Phi(\mathbf{x}_i)^T \Phi(\mathbf{x})+\frac{1}{y_m}-\sum_{i=1}^{M} \alpha_i y_i \Phi(\mathbf{x}_i)^T \Phi(\mathbf{x}_m)=0,
\tag{svm:26}\label{eq:svm_shpr}\end{align}$$ where $\mathbf{x}_m$ is a support
vector. Note from \eqref{eq:svm_opt11r} and \eqref{eq:svm_shpr} that SVMs
only depend on data samples through inner products in a feature space.
Thus, if we define a kernel function $k(\mathbf{x}_i, \mathbf{x}_j)$
that satisfies $$\begin{align}
k(\mathbf{x}_i, \mathbf{x}_j)=\Phi(\mathbf{x}_i)^T \Phi(\mathbf{x}_j),\nonumber\end{align}$$
and calculate the inner products by $k(\mathbf{x}_i, \mathbf{x}_j)$,
then we do not even need the mapping explicitly. This use of a kernel
function to avoid explicit mapping into a feature space is known as the
kernel trick. A choice of the kernel function is on your taste. Only
restriction is that the kernel function must be a proper inner product
(refer to Mercer’s condition for more detail). One example of a kernel
function is $$\begin{align}
k(\mathbf{x}_i, \mathbf{x}_j)= e^{-\frac{||\mathbf{x}_i - \mathbf{x}_j||^2}{2\sigma^2}}, \nonumber\end{align}$$
where $\sigma&amp;gt;0$ is a parameter. This is called the (Gaussian) radial
basis function kernel, or RBF kernel.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;sequential-minimal-optimization-smo&#34;&gt;Sequential minimal optimization (SMO)&lt;/h2&gt;

&lt;p&gt;In the most general version of SVMs, training is solving the quadratic
programming (QP) problem described as follows:&lt;/p&gt;

&lt;p&gt;$$\begin{align}
&amp;amp; \max_{\mathbf{\alpha}} \Psi(\mathbf{\alpha}) \nonumber\label{eq:smo_soft11}\\&lt;br /&gt;
\text{subject to } &amp;amp; \sum_{m=1}^{M} \alpha_m y_m =0, \tag{svm:27}\label{eq:smo_soft12}\\&lt;br /&gt;
&amp;amp; 0 \le \alpha_m \le C \text{ for all } m, \nonumber\label{eq:smo_soft13}\end{align}$$&lt;/p&gt;

&lt;p&gt;where $$\begin{align}
\Psi(\mathbf{\alpha}) = \sum_{m=1}^M \alpha_m - \frac{1}{2}\sum_{i=1}^M \sum_{j=1}^M \alpha_i \alpha_j y_i y_j k(\mathbf{x}_i, \mathbf{x}_j).
\tag{svm:28}\label{eq:smo_Psi}\end{align}$$ The solution of this problem must
satisfy the KKT conditions in \eqref{eq:svm_kkt}, which we now re-express
as $$\begin{array}{rl}
y_m u_m \ge 1 &amp;amp; \text{if } \alpha_m=0,\\&lt;br /&gt;
y_m u_m = 1 &amp;amp; \text{if } 0&amp;lt;\alpha_m&amp;lt;C,\\&lt;br /&gt;
y_m u_m \le 1 &amp;amp; \text{if } \alpha_m=C,
\end{array}
\label{eq:smo_kkt}$$ or equivalently&lt;/p&gt;

&lt;p&gt;$$\begin{matrix}
y_m e_m \ge 0 &amp;amp; \text{if } \alpha_m=0,\\&lt;br /&gt;
y_m e_m = 0 &amp;amp; \text{if } 0&amp;lt;\alpha_m&amp;lt;C,\\&lt;br /&gt;
y_m e_m \le 0 &amp;amp; \text{if } \alpha_m=C,\\&lt;br /&gt;
\end{matrix}
\tag{svm:29}\label{eq:smo_kkt2}$$&lt;/p&gt;

&lt;p&gt;where $u_m$ is the output of the SVM for the
$m$-th training sample, &lt;em&gt;i.e.&lt;/em&gt;, $$\begin{align}
u_m=\sum_{j=1}^{M} \alpha_j y_j k(\mathbf{x}_j, \mathbf{x}_m) + b,
\tag{svm:30}\label{eq:smo_u}\end{align}$$ and $e_m$ is an error for the $m$-th
training sample, defined as $$\begin{align}
e_m=u_m-y_m.
\tag{svm:31}\label{eq:smo_e}\end{align}$$&lt;/p&gt;

&lt;p&gt;Standard numerical QP solvers can be used to solve the QP problem above.
However, getting the right solution by using such methods is not that
easy because of several practical issues. One example of the issues in
practice is memory. The quadratic form of \eqref{eq:smo_Psi} involves a
matrix that contains $M^2$ elements, which could be too large to be
processed within the memory of a single machine. Think about the case
when $M=10000$. Assuming an element is 4 bytes in size, we need 400
Mbytes solely for the matrix.&lt;/p&gt;

&lt;p&gt;To avoid such an issue of numerical QP solvers, sequential minimal
optimization (SMO) has been proposed as an alternative way to solve the
QP problem of SVMs. The SMO breaks the QP problem into a series of
smallest possible sub-problems. Because of the constraint
\eqref{eq:smo_soft12}, we cannot optimize $\Psi(\mathbf{\alpha})$ with
respect to a single Lagrange multiplier $\alpha_m$ with all others
fixed. Thus, the smallest possible sub-optimization involves two
Lagrange multipliers. In a high level, the SMO is simply summarized as
follows:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;Pick two Lagrange multipliers $\alpha_i$ and $\alpha_j$.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Optimize $\Psi(\mathbf{\alpha})$ with respect to $\alpha_i$ and
$\alpha_j$, holding all other Lagrange multipliers fixed.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Repeat steps 1 and 2 until convergence.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;As long as at least one of the two Lagrange multipliers is selected to
be the one that violates the KKT conditions, the value of
$\Psi(\mathbf{\alpha})$ gets larger (&lt;em&gt;i.e.&lt;/em&gt;, improved) at each
iteration, and thus convergence is guaranteed. (Refer to Osuna’s theorem
for more detail.)&lt;/p&gt;

&lt;h3 id=&#34;optimization-with-respect-to-two-lagrange-multipliers&#34;&gt;Optimization with respect to two Lagrange multipliers&lt;/h3&gt;

&lt;p&gt;Assume that we are optimizing $\alpha_i$ and $\alpha_j$, while other
Lagrange multipliers are fixed. Because of \eqref{eq:smo_soft12}, we have
$$\begin{align}
y_i \alpha_i+y_j \alpha_j=y_i \alpha_i^{o}+y_j\alpha_j^{o}=-\sum_{m \neq i,j} \alpha_m^{o} y_m.
\tag{svm:32}\label{eq:smo_const}\end{align}$$ Here, the superscript $o$ on a value
means that the value is the old one that was obtained in the previous
iteration. We use this notation throughout the rest of a section.
Because of the relationship of $\alpha_i$ and $\alpha_j$, the feasible
range of $\alpha_j$ is defined as $$\begin{align}
L \le \alpha_j \le H \nonumber\end{align}$$ where $$L = \left\{
\begin{array}{rl}
\max(0,\alpha_j-\alpha_i) &amp;amp; \text{if } y_i \ne y_j\\&lt;br /&gt;
\max(C,\alpha_j+\alpha_i-C) &amp;amp; \text{if } y_i = y_j,
\end{array} \right.
\label{eq:bound_L}$$ and $$H = \left\{
\begin{array}{rl}
\min(0,\alpha_j-\alpha_i+C) &amp;amp; \text{if } y_i \ne y_j\\&lt;br /&gt;
\min(C,\alpha_j+\alpha_i) &amp;amp; \text{if } y_i = y_j.
\end{array} \right.
\label{eq:bound_H}$$&lt;/p&gt;

&lt;p&gt;Multiplying \eqref{eq:smo_const} by $y_i$, we have $$\begin{align}
\alpha_i+y_i y_j \alpha_j=\alpha_i^{o}+y_i y_j\alpha_j^{o}=-y_i\sum_{m \neq i,j} \alpha_m^{o} y_m.
\nonumber\label{eq:smo_const2}\end{align}$$ Let $s=y_i y_j$ and
$$\begin{align}
\gamma = \alpha_i^{o}+s \alpha_j^{o}= -y_i\sum_{m \neq i,j} \alpha_m^{o} y_m.
\tag{svm:33}\label{eq:smo_const3}\end{align}$$ Then, we can write $\alpha_i$ as
$$\begin{align}
\alpha_i=\gamma-s \alpha_j.
\tag{svm:34}\label{eq:smo_const4}\end{align}$$ Replacing $\alpha_i$ in
\eqref{eq:smo_Psi} with $\gamma-s\alpha_j$, we can express the objective
function $\Psi(\mathbf{\alpha})$ in terms of $\alpha_j$ as
$$\begin{align}
\Psi(\mathbf{\alpha}) = \frac{1}{2}\eta \alpha_j^2 + (y_j(e_i^{o}-e_j^{o})- \eta \alpha_j^{o})\alpha_j + \text{constant} \nonumber\end{align}$$
where $$\begin{align}
\eta = 2k(\mathbf{x}_i, \mathbf{x}_j)-k(\mathbf{x}_i, \mathbf{x}_i)-k(\mathbf{x}_j, \mathbf{x}_j).\nonumber\end{align}$$
Note $$\begin{align}
\frac{\partial^2 \Psi(\mathbf{\alpha})}{\partial \alpha_j^2}=\eta.\nonumber\end{align}$$&lt;/p&gt;

&lt;p&gt;If $\eta &amp;lt;0$, the objective function $\Psi(\mathbf{\alpha})$ is
maximized when&lt;/p&gt;

&lt;p&gt;$$\begin{align}
\frac{\partial \Psi(\mathbf{\alpha})}{\partial \alpha_j}=\eta \alpha_j+y_j(e_i^{o}-e_j^{o})- \eta \alpha_j^{o}=0.
\tag{svm:35}\label{eq:opt_alpha2}\end{align}
$$&lt;/p&gt;

&lt;p&gt;Let $\alpha_j^{n}$ be the value of
$\alpha_j$ that satisfies \eqref{eq:opt_alpha2}, that is,
$$\begin{align}
\alpha_j^{n}= \alpha_j^{o} + \frac{y_j(e_j^{o}-e_i^{o})}{\eta}.
\tag{svm:36}\label{eq:smo_alpha_2_update}\end{align}$$ If $\alpha_j^{n}$ is not in
the feasible range, the maximum is achieved at the boundary that is
close to $\alpha_j^{n}$, because $\Psi(\mathbf{\alpha})$ is a quadratic
in terms of $\alpha_j$. To sum up, the optimal value of $\alpha_j$,
denoted by $\alpha_j^{*}$ is:&lt;/p&gt;

&lt;p&gt;$$\alpha_j^{*} = \left\{
\begin{array}{rl}
H &amp;amp; \text{if } \alpha_j^{n} &amp;gt; H\\&lt;br /&gt;
\alpha_j^{n} &amp;amp; \text{if } L \le \alpha_j^{n} \le H\\&lt;br /&gt;
L &amp;amp; \text{if } \alpha_j^{n} &amp;lt; L.
\end{array} \right.
\label{eq:opt_alpha2_final}
$$&lt;/p&gt;

&lt;p&gt;If $\eta=0$, the objective function $\Psi(\mathbf{\alpha})$ is reduced
to $$\begin{align}
\Psi(\mathbf{\alpha}) = y_j(e_i^{o}-e_j^{o})\alpha_j + \text{constant}.\nonumber\end{align}$$
Since $\Psi(\mathbf{\alpha})$ is a line function in terms of $\alpha_j$,
we can achieve its maximum at one of the boundaries of the feasible
range: $L$ or $H$. In other words,&lt;/p&gt;

&lt;p&gt;$$
\alpha_{j}^{*} =
\left\{
\begin{matrix}
H &amp;amp; \text{if } y_{j}(e_{i}^{o}-e_{j}^{o})L &amp;lt; y_{j}(e_{i}^{o}-e_{j}^{o}) H \\&lt;br /&gt;
L &amp;amp; \text{if } y_{j}(e_i^{o}-e_{j}^{o})L &amp;gt; y_{j}(e_{i}^{o}-e_{j}^{o}) H.\\&lt;br /&gt;
\end{matrix} \right.
\label{eq:opt_alpha2_final2}
$$&lt;/p&gt;

&lt;p&gt;Occasionally, $\Psi(\mathbf{\alpha})$
could be the same at the both ends of the feasible range, which means
that we cannot make a progress through this optimization. In such a
case, we just set $\alpha_j^{*}=\alpha_j^{o}$.&lt;/p&gt;

&lt;p&gt;If $\eta&amp;gt;0$, the objective function $\Psi(\mathbf{\alpha})$ is convex
downward and thus there exists no solution. This case may occur with the
kernel that does not satisfy Mercer’s conditions.&lt;/p&gt;

&lt;p&gt;Meanwhile, from \eqref{eq:smo_const3} and \eqref{eq:smo_const4}, the value
of $\alpha_i^{*}$, &lt;em&gt;i.e.&lt;/em&gt;, the optimal value of $\alpha_i$, can be
obtained as: $$\begin{align}
\alpha_i^{*} = \alpha_i^{o} +s(\alpha_j^{o}-\alpha_j^{*}).\nonumber\end{align}$$&lt;/p&gt;

&lt;h3 id=&#34;update-after-optimization&#34;&gt;Update after optimization&lt;/h3&gt;

&lt;p&gt;Define $\Delta \alpha_i = \alpha_i^{*}-\alpha_i^{o}$ and
$\Delta \alpha_j = \alpha_j^{*}-\alpha_j^{o}$. Then, we can update the
values of $e_i$, $e_j$, and $b$ in terms of $\Delta\alpha_i$ and
$\Delta\alpha_j$.&lt;/p&gt;

&lt;p&gt;From \eqref{eq:smo_u} and \eqref{eq:smo_e}, it is easy to see that the new
value of an error $e_m$ is given by $$\begin{align}
e_m=e_m^{o}+\Delta\alpha_i y_i k(\mathbf{x}_i, \mathbf{x}_m) + \Delta\alpha_j y_j k(\mathbf{x}_j, \mathbf{x}_m) + \Delta b.\nonumber\end{align}$$
in which $\Delta b$ is the change in $b$, &lt;em&gt;i.e.&lt;/em&gt;,
$\Delta b = b - b^{o}$. In general, from the fact that $e_m=0$ for $m$
such that $0 &amp;lt; \alpha_m &amp;lt; C$, we can obtain $\Delta b$ as
$$\begin{align}
\Delta b=-\left( e_m^{o}+\Delta\alpha_i y_i k(\mathbf{x}_i, \mathbf{x}_m) + \Delta\alpha_j y_j k(\mathbf{x}_j, \mathbf{x}_m) \right).\nonumber\end{align}$$
Thus, in practice, we decide $$\Delta b = \left\{
\begin{array}{rl}
\Delta b_i &amp;amp; \text{if } 0&amp;lt;\alpha_i&amp;lt;C\\&lt;br /&gt;
\Delta b_j &amp;amp; \text{if } 0&amp;lt;\alpha_j&amp;lt;C\\&lt;br /&gt;
(\Delta b_i+\Delta b_j)/2 &amp;amp; \text{otherwise},
\end{array} \right.
\label{eq:delta_b}$$ where
$\Delta b_m=-\left( e_m^{o}+\Delta\alpha_i y_i k(\mathbf{x}_i, \mathbf{x}_m) + \Delta\alpha_j y_j k(\mathbf{x}_j, \mathbf{x}_m) \right)$.
Note that when both $\alpha_i$ and $\alpha_j$ are at boundaries
(&lt;em&gt;i.e.&lt;/em&gt;, 0 or $C$), any value between $\Delta b_i$ and $\Delta b_j$
satisfies the KKT conditions and thus we just take the average of
$\Delta b_i$ and $\Delta b_j$.&lt;/p&gt;

&lt;h3 id=&#34;how-to-choose-two-lagrange-multipliers-to-optimize&#34;&gt;How to choose two Lagrange multipliers to optimize&lt;/h3&gt;

&lt;p&gt;In order to accelerate convergence, we use some heuristics to choose two
Lagrange multipliers to jointly optimize. The basic idea is to choose
$\alpha_j$ in an outer loop that violates KKT conditions and $\alpha_i$
in an inner loop that can result in the largest $\Delta \alpha_j$.&lt;/p&gt;

&lt;p&gt;From \eqref{eq:smo_kkt2}, KKT conditions are violated when
$$
y_j e_j &amp;lt; 0  \text{ and } \alpha_j&amp;lt;C
\label{eq:smo_kkt_violated}
$$&lt;/p&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;p&gt;$$y_j e_j &amp;gt;0 \text{ and } \alpha_j&amp;gt;0.
\label{eq:smo_kkt_violated2}
$$&lt;/p&gt;

&lt;p&gt;When selecting $\alpha_i$, we do not
calculate the exact value of $\Delta \alpha_j$. Instead, we estimate it
by the absolute value of $e_j^{o}-e_i^{o}$ that comes in
\eqref{eq:smo_alpha_2_update}. That way, we can avoid evaluating
kernels, which is time-consuming.&lt;/p&gt;

&lt;p&gt;There are a few more minor heuristics that were introduced in the
original implementation of the SMO algorithm. For details, readers are
recommended to refer to Platt’s paper.&lt;/p&gt;

&lt;h3 id=&#34;practice-1&#34;&gt;Practice&lt;/h3&gt;

&lt;p&gt;SMO implementation&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import random

alpha = []
point = []
label = []
error = []
C = None
M = None
N = None
B = None
tol = 0.001
eps = 0.001
kernel = None
#----------------------------------------------------------
def default_kernel(p1, p2):
    ret = 0
    for q, w in zip(p1, p2):
        ret += q*w
    return ret
#----------------------------------------------------------
def nb_idx_error(i2 = None):
    if i2 == None:
        e2 = 0
    else:
        e2 = error[i2]

    idx_error = [(i, abs(error[i]-e2)) for i in range(len(alpha)) \
                if (alpha[i] &amp;gt; 0) and (alpha[i] &amp;lt; C)]
    return idx_error
#----------------------------------------------------------
def get_weight_linear():
    w = [0]*N
    for m in range(M):
        if (alpha[m] &amp;gt; 0):
            val = alpha[m]*label[m]
            for i in range(N):
                w[i] += val*point[m][i]

    return w    
#----------------------------------------------------------
def get_b():
    return B 
#----------------------------------------------------------
def get_alpha():
    return alpha 
#----------------------------------------------------------
def svm(x):
    svm_out = 0
    for m in range(M):
        svm_out += label[m]*alpha[m]*kernel(point[m], x)
    svm_out += B

    return svm_out    
#----------------------------------------------------------
def update_error(i1):
    global error

    error[i1] = svm(point[i1]) - label[i1]

    return error[i1]
#----------------------------------------------------------
def take_step(i1, i2):
    global B, error, alpha

    if (i1 == i2):
        return 0
    alph1 = alpha[i1]
    alph2 = alpha[i2]
    y1 = label[i1]
    y2 = label[i2]
    e1 = update_error(i1)
    e2 = error[i2]
    s = y1*y2

    L = 0
    H = 0
    if (y1 == y2):
        L = max(0, alph2 + alph1 - C)
        H = min(C, alph2 + alph1)
    else:
        L = max(0, alph2 - alph1)
        H = min(C, alph2 - alph1 + C)
    if (L == H):
        return 0

    k11 = kernel(point[i1], point[i1])
    k12 = kernel(point[i1], point[i2])
    k22 = kernel(point[i2], point[i2])
    eta = 2*k12 - k11 - k22
    if (eta &amp;lt; 0):
        a2 = alph2 + y2*(e2-e1)/float(eta)
        if (a2 &amp;lt; L):
            a2 = L
        elif (a2 &amp;gt; H):
            a2 = H
    elif (eta == 0):
        v = y2*(e1-e2)
        obj_L = v*L
        obj_H = v*H

        if (obj_L &amp;gt; obj_H-eps):
            a2 = L
        elif (obj_L &amp;lt; obj_H+eps):
            a2 = H
        else:
            a2 = alph2
    else:
        raise Exception(&#39;eta &amp;gt; 0: invalid kernel&#39;)

    if abs(a2-alph2) &amp;lt; eps*(a2+alph2+eps):
        return 0

    a1 = alph1 + s*(alph2-a2)

    delta1 = a1 - alph1
    delta2 = a2 - alph2

    # update b
    delta_b = 0
    delta_b1 = -( e1 + delta1*y1*kernel(point[i1],point[i1]) \
                + delta2*y2*kernel(point[i2],point[i1]) )
    delta_b2 = -( e2 + delta1*y1*kernel(point[i1],point[i2]) \
                + delta2*y2*kernel(point[i2],point[i2]) )
    if (a1&amp;gt;0) and (a1&amp;lt;C):
        delta_b = delta_b1
    elif (a2&amp;gt;0) and (a2&amp;lt;C):
        delta_b = delta_b2
    else:
        delta_b = (delta_b1+delta_b2)/2.0
    B += delta_b

    # update errors
    error[i1] += delta1*y1*kernel(point[i1], point[i1]) \
                + delta2*y2*kernel(point[i2], point[i1]) + delta_b
    error[i2] += delta1*y1*kernel(point[i1], point[i2]) \
                + delta2*y2*kernel(point[i2], point[i2]) + delta_b

    # update alphas
    alpha[i1] = a1
    alpha[i2] = a2

    return 1
#----------------------------------------------------------
def examine_example(i2):
    y2 = label[i2]
    alph2 = alpha[i2]
    e2 = update_error(i2)
    r2 = e2*y2
    # do something when there is a Lagrange multiplier that violates KKT conditions
    if (r2&amp;lt;-tol and alph2&amp;lt;C) or (r2&amp;gt;tol and alph2&amp;gt;0):
        idx = []
        idx_error = nb_idx_error(i2)
        if (len(idx_error) &amp;gt; 1):
            idx, error_dif = zip(*idx_error)
            i1 = idx[error_dif.index(max(error_dif))]
            if take_step(i1, i2):
                return 1
            idx = list(idx)
            random.shuffle(idx)

        for i1 in idx:
            if take_step(i1, i2):
                return 1

        seq = range(M)            
        random.shuffle(seq)
        for i1 in seq:
            if take_step(i1, i2):
                return 1

    return 0
#----------------------------------------------------------
def main():
    loop_cnt = 0
    num_changed = 0
    examine_all = 1

    # This while-loop terminates
    # when examine_all was 1 and num_changed becomes 0
    # i.e., when KKT conditions hold
    while (num_changed &amp;gt; 0) or (examine_all):
        num_changed = 0
        if (examine_all):
            for m in range(M):
                num_changed += examine_example(m)
        else:
            idx = []
            idx_error = nb_idx_error()
            if (len(idx_error) &amp;gt; 1):
                idx, error_ = zip(*idx_error)
            for i in idx:
                num_changed += examine_example(i)

        if (examine_all):
            examine_all = 0
        elif (num_changed == 0):
            examine_all = 1            

        loop_cnt += 1
        print &amp;quot;loop_cnt = &amp;quot; + str(loop_cnt)
#----------------------------------------------------------
def init(_point, _label, _c, _kernel = None):
    global point, label, C, M, N, B, kernel
    point = _point
    label = _label
    C = _c
    M = len(_label)
    N = len(_point[0])
    B = 0
    for m in range(M):
        alpha.append(0.0)
        error.append(-_label[m])

    if (_kernel == None):
        kernel = default_kernel
    else:
        kernel = _kernel
#----------------------------------------------------------
if __name__ == &amp;quot;__main__&amp;quot;:
    _point = [(6.4, 3.5), (7.4, 2.1), (5.0, 3.5), (5.5, 6), (5.9, 2.5), (5, 2), \
             (2.5, -1.9), (2.5, -1.9), (4.8, -6.1), (3, -6), (2.3, -0.5), (2.1, -2.3)]
    _label = (-1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1)
    init(_point, _label, 0.05)
    main()

    print repr(alpha)
    print repr(get_weight_linear())
    print B
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;SVM training with SMO&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
import smo

point = [(6.4, 3.5), (7.4, 2.1), (5.0, 3.5), (5.5, 6), (5.9, 2.5),\
         (5, 2), (2.5, -1.9), (4.8, -6.1), (3, -6), (2.3, -0.5), (2.1, -2.3)]
label = (-1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1)
C = 5.0
smo.init(point, label, C)
smo.main()

for p, y in zip(point, label):
    if (y == -1):
        plt.plot(p[0], p[1], &#39;go&#39;, markersize=10)
    else:
        plt.plot(p[0], p[1], &#39;rs&#39;, markersize=10)

w = smo.get_weight_linear()
b = smo.get_b()
x = np.arange(1,8,0.1)
a = -w[0]/w[1]
y0 = -b/w[1]
y = a * x + y0
plt.plot(x, y, &#39;k-&#39;, lw=2)
plt.savefig(&#39;svm_boundary.pdf&#39;)

import numpy as np
import matplotlib.pyplot as plt
import smo
import math
#---------------------------------------------------------------------
def my_kernel(x1, x2):
    x1 = np.array(x1)
    x2 = np.array(x2)
    r = np.linalg.norm(x1-x2)
    ret = math.exp(-pow(r,2)/(2.0*50))
    return ret 
#---------------------------------------------------------------------

point = [(6.4, 3.5), (7.4, 2.1), (5.0, 3.5), (5.5, 6), (5.9, 2.5),\
         (5, 2), (2.5, -1.9), (4.8, -6.1), (3, -6), (2.3, -0.5), (2.1, -2.3)]
label = (-1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1)
C = 20.0


smo.init(point, label, C, _kernel = my_kernel) # try C = 2.0
smo.main()

print &amp;quot;alpha = &amp;quot; + repr(smo.get_alpha())

for p, y in zip(point, label):
    y = smo.svm(p)
    print &amp;quot;y = &amp;quot; + str(y)
    if (y &amp;lt;= -1):
        plt.plot(p[0], p[1], &#39;go&#39;, markersize=10)
    elif (y &amp;gt;= 1):
        plt.plot(p[0], p[1], &#39;rs&#39;, markersize=10)
    else:
        plt.plot(p[0], p[1], &#39;bx&#39;, markersize=10)
plt.xlim(1, 8)
plt.savefig(&#39;svm_classification.pdf&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 6 is the result of the code above.&lt;br /&gt;

&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/svm_classification.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Classfication by SVM.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;&lt;p&gt;Notations here are inconsistent with the ones used in other
posts: $\mathbf{x}_{m}$ and $\mathbf{w}$ do not contain their
zeroth element, $x_{m,0}$ and $w_0$, respectively. This is intended
to be consistent with most of other SVM literature.&lt;/p&gt;
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Softmax Regression</title>
      <link>https://helix979.github.io/jkoo/post/ml-softmax/</link>
      <pubDate>Thu, 11 Feb 2016 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-softmax/</guid>
      <description>

&lt;p&gt;Softmax regression is a classification method that generalizes logistic
regression to multiclass problems in which possible outcomes are more
than two. For this reason, softmax regression is also called multinomial
logistic regression.&lt;/p&gt;

&lt;h3 id=&#34;generalization-from-logistic-regression&#34;&gt;Generalization from logistic regression&lt;/h3&gt;

&lt;p&gt;In logistic regression, we have modelled the conditional probabilities
as $$\begin{aligned}
P(y_{m}=1 \vert \mathbf{x}_{m})=\frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}_{m}}}
\end{aligned}
\tag{sm:1}\label{eq:cond1}
$$
 and
 $$\begin{aligned}
P(y_{m}=0 \vert \mathbf{x}_{m})=\frac{e^{-\mathbf{w}^T \mathbf{x}_{m}}}{1 + e^{-\mathbf{w}^T \mathbf{x}_{m}}},
\end{aligned}
\tag{sm:2}\label{eq:cond2}
$$
 which can be modified to
$$\begin{aligned}
P(y_{m}=1 \vert \mathbf{x}_{m})=\frac{e^{\mathbf{w}_1^T \mathbf{x}_{m}}}{e^{\mathbf{w}_1^T \mathbf{x}_{m}} + e^{(\mathbf{w}_1-\mathbf{w})^T \mathbf{x}_{m}}}
\end{aligned}
\tag{sm:3}\label{eq:cond3}
$$
 and
 $$\begin{aligned}
P(y_{m}=0 \vert \mathbf{x}_{m})=\frac{e^{(\mathbf{w}_1-\mathbf{w})^T \mathbf{x}_{m}}}{e^{\mathbf{w}_1^T \mathbf{x}_{m}} + e^{(\mathbf{w}_1-\mathbf{w})^T \mathbf{x}_{m}}}
\end{aligned}
\tag{sm:4}\label{eq:cond4}
$$&lt;/p&gt;

&lt;p&gt;Note that \eqref{eq:cond3} and
\eqref{eq:cond4} are made by multiplying
$\frac{e^{\mathbf{w}_1^T \mathbf{x}_{m}}}{e^{\mathbf{w}_1^T \mathbf{x}_{m}}}$
to \eqref{eq:cond1} and \eqref{eq:cond2}, respectively. Letting
$\mathbf{w}_0=\mathbf{w}_1-\mathbf{w}$, we can rewrite \eqref{eq:cond3}
and \eqref{eq:cond4} as follows: $$\begin{aligned}
P(y_{m}=k \vert \mathbf{x}_{m})=\frac{e^{\mathbf{w}_k^T \mathbf{x}_{m}}}{\sum_{i=0}^{1} e^{\mathbf{w}_i^T \mathbf{x}_{m}}} \text{ for } k=0,1.
\label{eq:cond5}\end{aligned}$$ Thus, the conditional probabilities can
be thought of as the ones that exponentiate weighted inputs and then
normalize them. In such a sense, softmax regression considers more than
two classes, say $K$ of them like $y_{m} \in {1,2,\ldots,K}$, with the
model of conditional probabilities in the following: $$\begin{aligned}
P(y_{m}=k \vert \mathbf{x}_{m})=\frac{e^{\mathbf{w}_k^T \mathbf{x}_{m}}}{\sum_{i=1}^{K} e^{\mathbf{w}_i^T \mathbf{x}_{m}}} \text{ for } k=1,2,\ldots,K.
\label{eq:cond6}\end{aligned}$$ Here,
$\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_K \in \mathbb{R}^{n+1}$
are the parameter vectors of our model.&lt;/p&gt;

&lt;h3 id=&#34;hypothesis-function&#34;&gt;Hypothesis function&lt;/h3&gt;

&lt;p&gt;Let a parameter matrix $\mathbf{W} \in \mathbb{R}^{(n+1) \times K}$ be&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
\mathbf{W} =
\begin{bmatrix} \mathbf{w}_1 &amp;amp; \mathbf{w}_2 &amp;amp; \cdots &amp;amp; \mathbf{w}_K
\end{bmatrix}.
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;Then, we define the hypothesis function of softmax regression, denoted
by $\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m}) \in \mathbb{R}^{K}$, as
$$\begin{aligned}
\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m}) =
\begin{bmatrix}
P(y_{m}=1 \vert \mathbf{x}_{m}) \\&lt;br /&gt;
P(y_{m}=2 \vert \mathbf{x}_{m}) \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
P(y_{m}=K \vert \mathbf{x}_{m}) \\&lt;br /&gt;
\end{bmatrix}
= \frac{1}{\sum_{i=1}^{K} e^{\mathbf{w}_i^T \mathbf{x}_{m}}}
\begin{bmatrix}
e^{\mathbf{w}_1^T \mathbf{x}_{m}} \\&lt;br /&gt;
e^{\mathbf{w}_2^T \mathbf{x}_{m}} \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
e^{\mathbf{w}_k^T \mathbf{x}_{m}} \\&lt;br /&gt;
\end{bmatrix}.\end{aligned}$$ The parameter matrix $\mathbf{W}$ should
be chosen in such a way that for $y_{m}=k$, the $k$-th element of
$\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m})$ becomes the largest among all
the elements.&lt;/p&gt;

&lt;p&gt;Note that $\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m})$ can be represented
as $$\begin{aligned}
\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m}) =
\sigma(\mathbf{W}^{T} \mathbf{x}_{m}),\end{aligned}$$ where
$\sigma(\mathbf{z})$ for
$\mathbf{z} = \begin{bmatrix} z_1 &amp;amp; z_2 &amp;amp; \ldots &amp;amp; z_K \end{bmatrix}^{T} \in \mathbb{R}^{K}$
is called the &lt;strong&gt;softmax function&lt;/strong&gt; and is given as $$\begin{aligned}
\sigma(\mathbf{z})=
\begin{bmatrix}
\sigma(\mathbf{z};1) \\&lt;br /&gt;
\sigma(\mathbf{z};2) \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
\sigma(\mathbf{z};K) \\&lt;br /&gt;
\end{bmatrix}\end{aligned}$$ with $$\begin{aligned}
\sigma(\mathbf{z};i) = \frac{e^{z_i}}{\sum_{k=1}^{K} e^{z_k}}.
\end{aligned}\tag{sm:5}\label{eq:sm_sigma}$$ The derivatives of the softmax
function have a nice property to remember:&lt;/p&gt;

&lt;p&gt;$$\begin{aligned}
\frac{\partial \sigma(\mathbf{z};i)}{\partial z_j}=
\left\{
\begin{matrix}
\sigma(\mathbf{z};i) (1 - \sigma(\mathbf{z};i)) &amp;amp; \text{if } i=j,\\&lt;br /&gt;
-\sigma(\mathbf{z};i)\sigma(\mathbf{z};j) &amp;amp; \text{if } i \neq j.\\&lt;br /&gt;
\end{matrix} \right.\end{aligned}
$$&lt;/p&gt;

&lt;h3 id=&#34;cost-function&#34;&gt;Cost function&lt;/h3&gt;

&lt;p&gt;The likelihood $l(\mathbf{W})$ of all data samples in the training set
can be represented as follows: $$\begin{aligned}
l(\mathbf{W})=\prod_{m=1}^{M}\prod_{k=1}^{K} \left( \frac{e^{\mathbf{w}_k^T \mathbf{x}_{m}}}{\sum_{i=1}^{K} e^{\mathbf{w}_i^T \mathbf{x}_{m}}} \right)^{\mathbf{1}_{m}(k)},
\label{eq:sm_likelihood}\end{aligned}$$ where the indicator function
$\mathbf{1}_{m}(k)$ is the same as &lt;a href=&#34;https://helix979.github.io/jkoo/jkoo/post/ml-logistic/#mjx-eqn-eqindicator&#34; target=&#34;_blank&#34;&gt;(logi:6)&lt;/a&gt;. Softmax
regression attempts to find the parameter matrix $\mathbf{W}$ that
maximizes the likelihood $l(\mathbf{W})$. This causes the $k$-th element
of $\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m})$ to be larger than any other
when $y_{m}=k$. Towards this end, the cost function is defined in a form
of a negative log-likelihood, as in logistic regression:&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
J(\mathbf{W})
&amp;amp;= -\frac{1}{M}\log l(\mathbf{W}) \\&lt;br /&gt;
&amp;amp;= -\frac{1}{M}\sum_{m=1}^{M} \sum_{k=1}^{K}
{\mathbf{1}_{m}(k)} \log\left( \frac{e^{\mathbf{w}_k^T \mathbf{x}_{m}}}{\sum_{i=1}^{K} e^{\mathbf{w}_i^T \mathbf{x}_{m}}} \right).
\end{aligned}\tag{sm:6}\label{eq:sm_cost}
$$&lt;/p&gt;

&lt;p&gt;It is worth noting that when $K=2$,
\eqref{eq:sm_cost} is equivalent to &lt;a href=&#34;https://helix979.github.io/jkoo/jkoo/post/ml-logistic/#mjx-eqn-eqlogistic_cost_multi&#34; target=&#34;_blank&#34;&gt;(logi:5)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;learning&#34;&gt;Learning&lt;/h3&gt;

&lt;p&gt;There is no known closed-form solution that minimizes the cost function
$J(\mathbf{W})$ in \eqref{eq:sm_cost}. Thus, we can apply the gradient
descent here as well. Using the notation in \eqref{eq:sm_sigma}, it is
easy to see:
$$\small
\begin{aligned}
\frac{\partial J(\mathbf{W})}{\partial \mathbf{w}_j}=
-\frac{1}{M}\sum_{m=1}^{M} \sum_{k=1}^{K}
\mathbf{1}_{m}(k) \left(\frac{1}{\sigma(\mathbf{W}^T \mathbf{x}_{m};k)} \cdot \frac{\partial \sigma(\mathbf{W}^T \mathbf{x}_{m};k)}{\partial (\mathbf{w}_j^T \mathbf{x}_{m})} \right) \mathbf{x}_{m}
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;Focusing on the difference between the term when $k=j$ and the terms
when $k \neq j$ of the inner summation, we continue to develop the
derivative equation as follows:&lt;/p&gt;

&lt;p&gt;$$\tiny
\begin{aligned}
\frac{\partial J(\mathbf{W})}{\partial \mathbf{w}_j}
&amp;amp;=-\frac{1}{M}\sum_{m=1}^{M} \left( \mathbf{1}_{m}(j)(1-\sigma(\mathbf{W}^T \mathbf{x}_{m};j))-\sum_{k \neq j} \mathbf{1}_{m}(k)\sigma(\mathbf{W}^T \mathbf{x}_{m};j) \right) \mathbf{x}_{m} \\&lt;br /&gt;
&amp;amp;=-\frac{1}{M}\sum_{m=1}^{M} \left( \mathbf{1}_{m}(j)-\left(\sum_{k=1}^{K} \mathbf{1}_{m}(k) \right)\sigma(\mathbf{W}^T \mathbf{x}_{m};j) \right) \mathbf{x}_{m} \\&lt;br /&gt;
&amp;amp;=-\frac{1}{M}\sum_{m=1}^{M} \left( \mathbf{1}_{m}(j)-\sigma(\mathbf{W}^T \mathbf{x}_{m};j) \right) \mathbf{x}_{m}.
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;Therefore, the gradient descent equations of softmax regression are
given by $$\begin{aligned}
\mathbf{w}_{j}^{(t+1)}=\mathbf{w}_{j}^{(t)}-\frac{\alpha}{M}\sum_{m=1}^{M} \left( \mathbf{1}_{m}(j)-\sigma(\mathbf{W}^{(t)T} \mathbf{x}_{m};j) \right) \mathbf{x}_{m}
\end{aligned}
$$
for all $j$.&lt;/p&gt;

&lt;h3 id=&#34;prediction&#34;&gt;Prediction&lt;/h3&gt;

&lt;p&gt;Denoting a new input vector by
$\mathbf{x} = \begin{bmatrix} 1 &amp;amp; x_1 &amp;amp; \ldots &amp;amp; x_n \end{bmatrix}^{T}$
and the solution by $\mathbf{W}^{*} =
\begin{bmatrix} \mathbf{w}_1^{*} &amp;amp; \mathbf{w}_2^{*} &amp;amp; \cdots &amp;amp; \mathbf{w}_K^{*} \end{bmatrix}$,
we decide $y=k$ when the largest element of
$\mathbf{h}_{\mathbf{W}^{*}}(\mathbf{x})$ is the $k$-th. In other words,
the prediction rule is given by: $$\begin{aligned}
y=\arg\max_{k}\left( \frac{e^{\mathbf{w}_k^{*T} \mathbf{x}}}{\sum_{i=1}^{K} e^{\mathbf{w}_i^{*T} \mathbf{x}}} \right).\end{aligned}$$&lt;/p&gt;

&lt;h3 id=&#34;practice&#34;&gt;Practice&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import random
import sys


n_data = 10000
r_x = 10
n_class = 3
n_batch = 10

#---------------------------------------------------------------------
def create_data(n):
    &amp;quot;&amp;quot;&amp;quot;
    This function will make a set of data such that
    a random number between c*r_x and (c+1)*r_x is given a label c. 
    &amp;quot;&amp;quot;&amp;quot;
    dataset = []
    for i in range(n):
        c = np.random.randint(n_class)
        x_1 = np.random.rand() * r_x + c*r_x
        y = c
        sample = [x_1, y]
        dataset.append(sample)
    random.shuffle(dataset)
    point_, label_ = zip(*dataset)
    _point_ = np.float32(np.array([point_]))
    _label_ = np.zeros([n_class, n])
    for i in range(len(label_)):
        _label_[label_[i]][i] = 1
    return _point_, _label_
#---------------------------------------------------------------------

# Create a dataset for training
point, label = create_data(n_data)

# Placeholders to take data in
x = tf.placeholder(tf.float32, [1, None])
y = tf.placeholder(tf.float32, [n_class, None])

# Write a model
w_1 = tf.Variable(tf.random_uniform([n_class, 1], -1.0, 1.0))
w_0 = tf.Variable(tf.random_uniform([n_class, 1], -1.0, 1.0))
y_hat = tf.nn.softmax(w_1 * x + w_0)
cost = -tf.reduce_sum(y*tf.log(y_hat))/n_batch
optimizer = tf.train.GradientDescentOptimizer(0.001)
train = optimizer.minimize(cost)

label_hat_ = tf.argmax(y_hat,0)
correct_cnt = tf.equal(tf.argmax(y,0), label_hat_)
accuracy = tf.reduce_mean(tf.cast(correct_cnt, &amp;quot;float&amp;quot;))


sess = tf.InteractiveSession()

# Initialize variables
init = tf.initialize_all_variables()
sess.run(init)

# Learning
step = 0
while 1:
    try:
        step += 1
        if (n_data == n_batch): # Gradient descent
            start = 0
            end = n_data
        else: # Stochastic gradient descent
            start = step * n_batch
            end = start + n_batch
            if (start &amp;gt;=n_data) or (end &amp;gt;=n_data):
                break
        batch_xs = point[:, start:end]
        batch_ys = label[:, start:end]

        train.run(feed_dict={x: batch_xs, y: batch_ys})

        if step % 10 == 0:
            print step
            print w_1.eval()
            print w_0.eval()

    # With gradient descent, ctrl+c will stop training
    except KeyboardInterrupt:
        break


# Create another dataset for test
point_t, label_t = create_data(100)
rate = accuracy.eval(feed_dict={x: point_t, y: label_t})
print &amp;quot;\n\n accuracy = %s\n&amp;quot; % (rate)

# Plot the test results
plt.plot(point_t[0,:], label_hat_.eval(feed_dict={x: point_t}), &#39;o&#39;)
plt.grid()
plt.ylim(-1, n_class)

xt = range(0, n_class*10+1, 10)
yt = range(-1, n_class, 1)
plt.step(xt, yt, &#39;r--&#39;)

plt.savefig(&#39;softmax_test.pdf&#39;)

sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Figure 1 is what we may get from the code above.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/softmax_test.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Blue dots that are not on the red line indicate classification errors.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;

</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://helix979.github.io/jkoo/post/ml-logistic/</link>
      <pubDate>Tue, 09 Feb 2016 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-logistic/</guid>
      <description>

&lt;p&gt;Logistic regression is a classification method for analyzing a set of
data in which there are one or more independent variables (inputs) that
determine a binary outcome $0$ or $1$.&lt;/p&gt;

&lt;h3 id=&#34;hypothesis-function&#34;&gt;Hypothesis function&lt;/h3&gt;

&lt;p&gt;Suppose that we are given $M$ data samples, denoting the $m$-th
observation pair of an input vector and an output by
$\mathbf{x}_{m} = \begin{bmatrix} x_0 &amp;amp; x_1 &amp;amp; \cdots &amp;amp; x_n \end{bmatrix}^{T} \in \mathbb{R}^{n+1}$
and $y_{m} \in {0,1}$, respectively. We call $\mathbf{x}_{m}$ a
positive sample if $y_{m}=1$ and a negative sample if $y_{m}=0$.&lt;/p&gt;

&lt;p&gt;Now, consider $P(y_{m} \vert \mathbf{x}_{m})$, a conditional probability
of $y_{m}$ given $\mathbf{x}_{m}$. The &lt;strong&gt;hypothesis function&lt;/strong&gt;
$h_{\mathbf{w}}(\mathbf{x}_{m})$ of logistic regression models the
conditional probability of a positive sample (&lt;em&gt;i.e.&lt;/em&gt;, $y_{m}=1$) as&lt;/p&gt;

&lt;p&gt;$$
P(y_{m}=1 \vert \mathbf{x}_{m})=h_{\mathbf{w}}(\mathbf{x}_{m}).
$$&lt;/p&gt;

&lt;p&gt;Here, the hypothesis function $h_{\mathbf{w}}(\mathbf{x}_{m})$ is
defined using the &lt;strong&gt;sigmoid function&lt;/strong&gt; $s(z)=\frac{1}{1 + e^{-z}}$
as follows:&lt;/p&gt;

&lt;p&gt;$$
h_{\mathbf{w}}(\mathbf{x}_{m})=s(\mathbf{w}^T \mathbf{x}_{m})=\frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}_{m}}}.
$$&lt;/p&gt;

&lt;p&gt;As shown in Figure 1, the sigmoid function $s(z)$ is a
‘S’-shape curve that converges to 1 as $z \rightarrow \infty$ and 0 as
$z \rightarrow -\infty$. Since $h_{\mathbf{w}}(\mathbf{x}_{m})$ attempts
to model a probability, the property of the sigmoid function $s(z)$ that
is bounded between 0 and 1 is a good fit for our purpose.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/sigmoid.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;sigmoid function $s(z)=\frac{1}{1 &amp;#43; e^{-z}}$.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;What is next is to choose $\mathbf{w}$ well so that we can have a high
value of $h_{\mathbf{w}}(\mathbf{x}_{m})$ for $\mathbf{x}_{m}$ that
corresponds to $y_{m}=1$, and a low value for $\mathbf{x}_{m}$ that
corresponds to $y_{m}=0$.&lt;/p&gt;

&lt;h3 id=&#34;cost-function&#34;&gt;Cost function&lt;/h3&gt;

&lt;p&gt;Since $P(y_{m}=1 \vert \mathbf{x}_{m})=h_{\mathbf{w}}(\mathbf{x}_{m})$
by our definition, we have&lt;/p&gt;

&lt;p&gt;$$
P(y_{m}=0 \vert \mathbf{x}_{m})=1-h_{\mathbf{w}}(\mathbf{x}_{m}).
$$&lt;/p&gt;

&lt;p&gt;For all the observations in the training set, the likelihood
$l(\mathbf{w})$ can then be written as&lt;/p&gt;

&lt;p&gt;$$
l(\mathbf{w})=\prod_{m=1}^{M} h_{\mathbf{w}}(\mathbf{x}_{m})^{y_{m}} ( 1 - h_{\mathbf{w}}(\mathbf{x}_{m}) )^{1-{y_{m}}}.
\tag{logi:1}\label{eq:lr_likelihood}
$$&lt;/p&gt;

&lt;p&gt;Logistic regression chooses the
parameter $\mathbf{w}$ that maximizes the likelihood of observing the
samples, represented in \eqref{eq:lr_likelihood}. Thus, we define the
&lt;strong&gt;cost function&lt;/strong&gt; $J(\mathbf{w})$ as&lt;/p&gt;

&lt;p&gt;$$\scriptsize
\begin{aligned}
J(\mathbf{w})
&amp;amp;= -\frac{1}{M}\log l(\mathbf{w}) \\&lt;br /&gt;
&amp;amp;= -\frac{1}{M}\sum_{m=1}^{M} \left( y_{m} \log h_{\mathbf{w}}(\mathbf{x}_{m}) +  ( 1-{y_{m}}) \log( 1 - h_{\mathbf{w}}(\mathbf{x}_{m}) ) \right).
\end{aligned}\tag{logi:2}\label{eq:lr_cost}
$$&lt;/p&gt;

&lt;p&gt;Note that minimizing \eqref{eq:lr_cost} is equivalent to maximizing
\eqref{eq:lr_likelihood}.&lt;/p&gt;

&lt;h3 id=&#34;learning&#34;&gt;Learning&lt;/h3&gt;

&lt;p&gt;The solution $\mathbf{w}^{*}$ of logistic regression can be obtained by
minimizing the cost function in \eqref{eq:lr_cost}. Again, such a
minimization can be done by using the gradient descent. Hence, we first
obtain the partial derivatives of $J(\mathbf{w})$ as follows:&lt;/p&gt;

&lt;p&gt;$$\tiny
\begin{aligned}
&amp;amp;\frac{\partial J(\mathbf{w})}{\partial w_j} \\&lt;br /&gt;
&amp;amp;=-\frac{1}{M} \sum_{m=1}^M \left( y_{m} \frac{1}{s(\mathbf{w}^{T} \mathbf{x}_{m})} - (1 - y_{m})\frac{1}{1-s(\mathbf{w}^{T} \mathbf{x}_{m})} \right) \frac{\partial}{\partial w_j}s(\mathbf{w}^{T} \mathbf{x}_{m}) \\&lt;br /&gt;
&amp;amp;=-\frac{1}{M} \sum_{m=1}^M \left( y_{m} \frac{1}{s(\mathbf{w}^{T} \mathbf{x}_{m})} - (1 - y_{m})\frac{1}{1-s(\mathbf{w}^{T} \mathbf{x}_{m})} \right) s(\mathbf{w}^{T} \mathbf{x}_{m})(1-s(\mathbf{w}^{T} \mathbf{x}_{m}))\frac{\partial}{\partial w_j}\mathbf{w}^{T} \mathbf{x}_{m}\\&lt;br /&gt;
&amp;amp;=-\frac{1}{M} \sum_{m=1}^M \left( y_{m} (1-s(\mathbf{w}^{T} \mathbf{x}_{m})) - (1 - y_{m})s(\mathbf{w}^{T} \mathbf{x}_{m}) \right)x_{m,j} \\&lt;br /&gt;
&amp;amp;=\frac{1}{M} \sum_{m=1}^M (h_{\mathbf{w}} (\mathbf{x}_{m})- y_{m})x_{m,j}.
\end{aligned}\label{eq:logistic_derivative}
$$&lt;/p&gt;

&lt;p&gt;It is worth noting that
the sigmoid function $s(z)$ has an easy-to-calculate derivative:
$$\begin{aligned}
\frac{ds(z)}{dz}=s(z)(1-s(z)).\end{aligned}$$ Now, the gradient descent
equations for logistic regression are&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
w_{j}^{(t+1)}
&amp;amp;= w_{j}^{(t)}-\frac{\alpha}{M} \sum_{m=1}^{M} (h_{\mathbf{w}^{(t)}} (\mathbf{x}_{m})- y_{m})x_{m,j} \\&lt;br /&gt;
&amp;amp;= w_{j}^{(t)}-\frac{\alpha}{M} \sum_{m=1}^{M} \left(\frac{1}{1 + e^{-\mathbf{w}^{(t)T} \mathbf{x}_{m}}}- y_{m} \right)x_{m,j}
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;for all $j$.&lt;/p&gt;

&lt;h3 id=&#34;prediction&#34;&gt;Prediction&lt;/h3&gt;

&lt;p&gt;Given a new input vector
$\mathbf{x} = \begin{bmatrix} 1 &amp;amp; x_1 &amp;amp; \ldots &amp;amp; x_n \end{bmatrix}^{T}$,
we should predict $y$ in the following rule.&lt;/p&gt;

&lt;p&gt;$$
y = \left\{
\begin{matrix}
1 &amp;amp; \text{if } h_{\mathbf{w}^{*}} (\mathbf{x}) \ge 0.5,\\&lt;br /&gt;
0 &amp;amp; \text{if } h_{\mathbf{w}^{*}} (\mathbf{x}) &amp;lt; 0.5.\\&lt;br /&gt;
\end{matrix} \right.
\tag{logi:3}\label{eq:lr_decision}
$$&lt;/p&gt;

&lt;p&gt;That way we can achieve the minimum error rate in classification. (Why? Refer to Bayes decision rule for detailed
reason.)&lt;/p&gt;

&lt;p&gt;Since
$h_{\mathbf{w}^{*}} (\mathbf{x})=s\left(\mathbf{w}^{*T} \mathbf{x}\right)$
and $s(z) \ge 0.5 $ when $z \ge 0$, the decision rule in
\eqref{eq:lr_decision} is equivalent to&lt;/p&gt;

&lt;p&gt;$$
y = \left\{
\begin{matrix}
1 &amp;amp; \text{if } \mathbf{w}^{*T} \mathbf{x} \ge 0,\\&lt;br /&gt;
0 &amp;amp; \text{if } \mathbf{w}^{*T} \mathbf{x} &amp;lt; 0.\\&lt;br /&gt;
\end{matrix}
\right.
\label{eq:lr_decision2}
$$&lt;/p&gt;

&lt;p&gt;Note that the solution to the linear equation
$\mathbf{w}^{*T} \mathbf{x}=0$ is the decision boundary separating the
two predicted classes. Figure 2 shows an example
where the decision boundary is a line.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/logistic.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Decision boundary (the blue line) learned by logistic regression separates the data samples, where the green dots and red x’s belong to a different class.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;h3 id=&#34;practice&#34;&gt;Practice&lt;/h3&gt;

&lt;p&gt;Figure 2 can be obtained by the following.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

n_data = 50
r_x = 10
noise_std = 5

class0 = []
class1 = []
x = None
y = None
#--------------------------------------------------------
def create_data():
    global class0, class1, x, y

    slope_1, y_intercept_1 = 1, 5
    slope_2, y_intercept_2 = 1, 0

    for i in range(n_data):
        x_1 = np.random.rand() * r_x
        x_2 = slope_1 * x_1 + y_intercept_1 + (np.random.rand() - 0.5) * noise_std
        class0.append([x_1, x_2, 0])

        x_1 = np.random.rand() * r_x
        x_2 = slope_2 * x_1 + y_intercept_2 + (np.random.rand() - 0.5) * noise_std
        class1.append([x_1, x_2, 1])

    dataset = sorted(class0+class1, key=lambda data: data[0])
    x1, x2, y = zip(*dataset)
    x = np.vstack((np.float32(np.array(x1)), np.float32(np.array(x2))))
    y = np.array(y)
#--------------------------------------------------------
def draw(w, w_0):
    xr = np.arange(0,r_x, 0.1)
    yr  = -(w[0][0] * xr + w_0[0] )/w[0][1]

    for x1, x2, y in class0:
        plt.plot(x1, x2, &#39;go&#39;, markersize=10)

    for x1, x2, y in class1:
        plt.plot(x1, x2, &#39;rx&#39;, markersize=10, mew=2)

    plt.plot(xr, yr, lw=2)
    plt.xlim(0, r_x)
    plt.xlabel(&#39;$x_1$&#39;, fontsize=20)
    plt.ylabel(&#39;$x_2$&#39;, fontsize=20)
    plt.gcf().subplots_adjust(bottom=0.15)
    plt.savefig(&#39;logistic.pdf&#39;)
#--------------------------------------------------------

# Create data
# x of shape [2, n_data], y of shape [1, n_data]
create_data()

# Write a model
w = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))
w_0 = tf.Variable(tf.random_uniform([1], -1.0, 1.0))
z = tf.matmul(w, x) + w_0   # w_0 is broadcasted

y_hat = 1 / (1+tf.exp(-z))
cost = -tf.reduce_sum(y*tf.log(y_hat) + (1-y)*(tf.log(1-y_hat))) / len(y)

optimizer = tf.train.GradientDescentOptimizer(0.001)
train = optimizer.minimize(cost)

sess=tf.InteractiveSession()

# Initialize variables
init = tf.initialize_all_variables()
sess.run(init)

# Training
step = 0
while 1:
    try:
        step += 1
        sess.run(train)
        if step % 100 == 0:
            print step, sess.run(cost), sess.run(w), sess.run(w_0)

    # Ctrl+c will stop training
    except KeyboardInterrupt:
        break

# Plot the sample data and decision boundary
draw(sess.run(w), sess.run(w_0))

sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;one-versus-rest-handling-multiclass-classification&#34;&gt;One versus rest: handling multiclass classification&lt;/h3&gt;

&lt;p&gt;What if there are more than two classes to classify? What is called the
one-versus-rest strategy is a way of extending any binary classifier
(including the Logistic regression classifier) to handle multiclass
classification problems. The one-versus-rest strategy changes a
multiclass classification into multiple binary classifications, training
a single classifier per class. The data samples that belongs to a class
are treated as positive samples of the class and all others as negative
samples.&lt;/p&gt;

&lt;p&gt;Consider $y_m={1,2,\ldots,K}$, &lt;em&gt;i.e.&lt;/em&gt;, we have to decide one out
of $K$ classes. Using the Logistic regression as an example, we can make
a binary classifier for the class of $y_m=k$ with a corresponding
parameter vector $\mathbf{w}_k \in \mathbb{R}^{n+1}$ as&lt;/p&gt;

&lt;p&gt;$$
\begin{aligned}
\mathbf{h}_{\mathbf{w}_k}(\mathbf{x}_{m}) =\frac{1}{1 + e^{-\mathbf{w}_k^T \mathbf{x}_{m}}}  \text{ for } k=1,2,\ldots,K.
\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;The hypothesis function
$\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m}) \in \mathbb{R}^{K}$ of the
multiclass classification problem can then be expressed as
$$\begin{aligned}
\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m}) =
\begin{bmatrix}
\mathbf{h}_{\mathbf{w}_1}(\mathbf{x}_{m}) \\&lt;br /&gt;
\mathbf{h}_{\mathbf{w}_2}(\mathbf{x}_{m}) \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
\mathbf{h}_{\mathbf{w}_K}(\mathbf{x}_{m}) \\&lt;br /&gt;
\end{bmatrix},\end{aligned}
$$&lt;/p&gt;

&lt;p&gt;where $\mathbf{W}$ is a parameter matrix
given as $$\begin{aligned}
\mathbf{W} =
\begin{bmatrix} \mathbf{w}_1 &amp;amp; \mathbf{w}_2 &amp;amp; \cdots &amp;amp; \mathbf{w}_K \end{bmatrix}.\end{aligned}$$
Note that for instance, when $y_m=1$, the first element of
$\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m})$, namely
$\mathbf{h}_{\mathbf{w}_1}(\mathbf{x}_{m})$, should be high and the
other elements should be low, since samples of the class $y_m=1$ are
treated as positives only for the class $y_m=1$ and as negatives for the
classes $y_m=2,3,\ldots, K$. Roughly speaking, this is achieved by
choosing $\mathbf{W}$ in such a way that
$\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m})$ is made as close to
$\mathbf{e}_k$ as possible when $y_m=k$, where $\mathbf{e}_k$ is the
$k$-th standard basis vector in $\mathbb{R}^{K}$, that is,
$$\begin{aligned}
\mathbf{e}_1 = \begin{bmatrix} 1 \\&lt;br /&gt;
0 \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
0
\end{bmatrix},
\mathbf{e}_2 = \begin{bmatrix} 0 \\&lt;br /&gt;
1 \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
0
\end{bmatrix},
\ldots,
\mathbf{e}_K = \begin{bmatrix} 0 \\&lt;br /&gt;
0 \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
1
\end{bmatrix}.
\end{aligned}\tag{logi:4}\label{eq:std_basis}$$ Formally, the optimal $\mathbf{W}$
is chosen when we minimize the following cost function:&lt;/p&gt;

&lt;p&gt;$$\tiny
\begin{aligned}
J(\mathbf{W})
= -\frac{1}{M}\sum_{m=1}^{M} \sum_{k=1}^{K}
\left( \mathbf{1}_{m}(k) \log h_{\mathbf{w}_k}(\mathbf{x}_{m}) +  ( 1-\mathbf{1}_{m}(k)) \log( 1 - h_{\mathbf{w}_k}(\mathbf{x}_{m}) ) \right),
\end{aligned}\tag{logi:5}\label{eq:logistic_cost_multi}
$$&lt;/p&gt;

&lt;p&gt;where $\mathbf{1}_{m}(k)$
is an indicator function, defined as&lt;/p&gt;

&lt;p&gt;$$\begin{aligned}
\mathbf{1}_{m}(k) =
\left\{
\begin{matrix}
1 &amp;amp; \text{if } y_{m}=k,\\&lt;br /&gt;
0 &amp;amp; \text{if } y_{m} \neq k.
\end{matrix} \right.
\end{aligned}\tag{logi:6}\label{eq:indicator}
$$&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://helix979.github.io/jkoo/post/ml-linear_regression/</link>
      <pubDate>Tue, 02 Feb 2016 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-linear_regression/</guid>
      <description>

&lt;p&gt;Linear regression is a means of modeling the relationship between one or
more independent variables (inputs) and a single dependent variable (an
output) by fitting a linear equation to observed data.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;hypothesis-function&#34;&gt;Hypothesis function&lt;/h2&gt;

&lt;p&gt;Given the $m$-th observation with inputs $\{ x_{m,1}, x_{m,2}, \ldots, x_{m,n} \}$
where
$x_{m,j} \in \mathbb{R}, \forall j$ and an output $y_m \in \mathbb{R}$,
we define an input vector as&lt;/p&gt;

&lt;p&gt;$$
\mathbf{x}_m=
\begin{bmatrix}
x_{m,0} \\&lt;br /&gt;
x_{m,1} \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
x_{m,n}
\end{bmatrix} \in \mathbb{R}^{n+1},
\label{eq:input_vector}
$$&lt;/p&gt;

&lt;p&gt;where we always set $x_{m,0}=1$, which is called a &lt;strong&gt;bias input&lt;/strong&gt; and considered here for notational
convenience. The goal of linear regression is to find an estimate
$\hat{y}_m=h_{\mathbf{w}}(\mathbf{x}_{m})$ of the output $y_{m}$ that is
of the form:&lt;/p&gt;

&lt;p&gt;$$
h_{\mathbf{w}}(\mathbf{x}_{m})=\mathbf{w}^T \mathbf{x}_{m}=w_0 + w_1 x_{m,1} + \ldots +w_n x_{m,n}.
$$&lt;/p&gt;

&lt;p&gt;We call $h_{\mathbf{w}}(\cdot)$ a &lt;strong&gt;hypothesis function&lt;/strong&gt;. By well
choosing the value of $\mathbf{w}$, we want
$h_{\mathbf{w}}(\mathbf{x}_{m})$ as close to $y_{m}$ as possible for
$m=1,2,\ldots,M$. Again, $M$ is the number of observations in the
training set.&lt;/p&gt;

&lt;p&gt;Note that when $n=1$, the hypothesis function is represented as&lt;/p&gt;

&lt;p&gt;$$
h_{\mathbf{w}}(\mathbf{x}_{m})=w_0 + w_1 x_{m,1},
$$&lt;/p&gt;

&lt;p&gt;which is the form of a straight line. Thus, all we need to do here is to find the slope
$w_1$ and the $y$-intercept $w_0$ of a straight line that fits best to
given points ${(x_{m,1}, y_{m})}_{m=1}^{M}$. This case is called
simple linear regression. Figure 1 shows an example of the simple linear regression.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/simple_linear.png&#34; alt=&#34;The blue dots represent the training data $\{(x_{m,1}, y_{m})\}_{m=1}^{M}$. The result of the simple linear regression is the red line&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Simple linear regression.&lt;/h4&gt;
        &lt;p&gt;
        The blue dots represent the training data $\{(x_{m,1}, y_{m})\}_{m=1}^{M}$. The result of the simple linear regression is the red line
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;cost-function&#34;&gt;Cost function&lt;/h2&gt;

&lt;p&gt;We need a measure of how &amp;ldquo;well&amp;rdquo; we have selected the value of
$\mathbf{w}$. To this end, the &lt;strong&gt;cost function&lt;/strong&gt; $J(\mathbf{w})$ can
be defined as&lt;/p&gt;

&lt;p&gt;$$
J(\mathbf{w})=\frac{1}{M} \sum_{m=1}^{M} \left(h_{\mathbf{w}}(\mathbf{x}_{m})-y_{m} \right)^{2}.
\tag{lr:1}\label{eq:mse}
$$&lt;/p&gt;

&lt;p&gt;Saying the value of $h_{\mathbf{w}}(\mathbf{x}_{m})-y_{m}$ is an error, the cost function
above is the mean of squared errors. Then, the best value of
$\mathbf{w}$, &lt;em&gt;i.e.&lt;/em&gt;, the solution $\mathbf{w}^{*}$, is chosen as the
one that minimizes the cost function $J(\mathbf{w})$ in \eqref{eq:mse}.
Such a solution is said to be optimal in the sense of minimizing mean-squared errors (MMSE).&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;learning&#34;&gt;Learning&lt;/h2&gt;

&lt;p&gt;Formally, the solution $\mathbf{w}^{*}$ is obtained as
$$
\mathbf{w}^{*}=\arg \min_{\mathbf{w}} J(\mathbf{w}).
$$&lt;/p&gt;

&lt;p&gt;This can be solved numerically by using the gradient descent. Since
$$
J(\mathbf{w})=\frac{1}{M} \sum_{m=1}^{M} \left(w_0 + w_1 x_{m,1} + \ldots +w_n x_{m,n}-y_{m} \right)^{2},
$$&lt;/p&gt;

&lt;p&gt;we have
$$
\frac{\partial}{\partial w_j} J(\mathbf{w})= \frac{2}{M} \sum_{m=1}^{M} \left(w_0 + w_1 x_{m,1} + \ldots +w_n x_{m,n}-y_{m} \right)x_{m,j}.
$$
Therefore, from &lt;a href=&#34;https://helix979.github.io/jkoo/jkoo/post/ml-optimization/#mjx-eqn-eqopt&#34; target=&#34;_blank&#34;&gt;(opt:1)&lt;/a&gt;, the gradient descent equations for
linear regression are&lt;/p&gt;

&lt;p&gt;$$
w_{j}^{(t+1)}=w_{j}^{(t)}-\alpha\frac{2}{M} \sum_{m=1}^{M} \left(w_0^{(t)} + w_1^{(t)} x_{m,1} + \ldots +w_n^{(t)} x_{m,n}-y_{m} \right)x_{m,j}
$$&lt;/p&gt;

&lt;p&gt;for all $j$.&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;prediction&#34;&gt;Prediction&lt;/h2&gt;

&lt;p&gt;After we have found the solution $\mathbf{w}^{*}$, if an additional input vector
$\mathbf{x} = \begin{bmatrix} 1 &amp;amp; x_1 &amp;amp; \ldots &amp;amp; x_n \end{bmatrix}^{T}$
is given, its corresponding output $y$ can be predicted as follows:&lt;/p&gt;

&lt;p&gt;$$
y=h_{\mathbf{w}^{*}}(\mathbf{x})=w_0^{*} + w_1^{*} x_1 + \ldots +w_n^{*} x_n = \mathbf{w}^{*T} \mathbf{x}.
$$&lt;/p&gt;

&lt;p&gt;&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;practice&#34;&gt;Practice&lt;/h2&gt;

&lt;p&gt;The following code show an example of the simple lienar gression.
Data used for training is created by adding noises around the straight line of y-intercept=10 and slope=1.
The output would be like Figure 2.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

n_data = 100
r_x = 100
noise_std = 50
x = None
y = None
y_ideal = None
#----------------------------------------------------------------
def create_data():
    global x, y, y_ideal
    y_intercept = 10
    slope = 1
    x = np.float32(np.random.rand(n_data)) * r_x
    x = np.sort(x)
    noise = (np.float32(np.random.rand(n_data)) - 0.5) * noise_std
    y_ideal = slope * x + y_intercept
    y = y_ideal + noise
#----------------------------------------------------------------
def draw(w1, w0):
    y_hat = w1 * x + w0

    plt.plot(x[:], y_ideal[:], &#39;r--&#39;, \
             x[:], y[:], &#39;b.&#39;,\
             x[:], y_hat[:], &#39;g--&#39;)

    plt.xlim(0, r_x)
    plt.legend([&#39;ideal&#39;, &#39;data&#39;, &#39;regression&#39;], loc=&#39;best&#39;)
    plt.savefig(&#39;linear_one.pdf&#39;)
#----------------------------------------------------------------

# Create data for training
create_data()

# Model
w_1 = tf.Variable(np.random.rand())
w_0 = tf.Variable(np.random.rand())
m = w_1 * x
h = m + w_0   # Note w_0 is broadcasted to be the same shape as m

J = tf.reduce_mean(tf.square(h - y))
optimizer = tf.train.GradientDescentOptimizer(0.0001)
train = optimizer.minimize(J)

init = tf.initialize_all_variables()
sess=tf.InteractiveSession()
sess.run(init)

# Learning
step = 0
while 1:
    try:
        step += 1
        sess.run(train)
        if step % 100 == 0:
            print step, J.eval(), w_1.eval(), w_0.eval()

    # Ctrl+c will stop training
    except KeyboardInterrupt:
        break

# Plot the result
draw(w_1.eval(), w_0.eval())

sess.close()
&lt;/code&gt;&lt;/pre&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/linear_one.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Simple linear gression&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;The following code is doing the similar thing as above, but this time we consider the case of $n=2$.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
from mpl_toolkits.mplot3d import Axes3D

N_data = 20
R_x = 100
noise_std = 100
xs = None
ys = None
zs = None
zs_ideal = None
X = None
Y = None
#----------------------------------------------------------------
def create_data():
    global xs, ys, zs, zs_ideal, X, Y
    slope_x = -2
    slope_y = 1
    z_intercept = 4
    x = np.random.rand(N_data) * R_x
    x = np.sort(x)
    y = np.random.rand(N_data) * R_x
    y = np.sort(y)
    X, Y = np.meshgrid(x, y)
    zf = lambda x, y: slope_x * x + slope_y * y + z_intercept
    xs = np.float32(np.ravel(X))
    ys = np.float32(np.ravel(Y))
    zs_ideal = np.array([zf(x,y) for x,y in zip(xs, ys)])
    zs = zs_ideal + (np.float32(np.random.rand(len(zs_ideal))) - 0.5) * noise_std
#----------------------------------------------------------------
def draw(w_est, w_0_est):
    zf_est = lambda x, y: w_est[0][0] * x + w_est[0][1] * y + w_0_est
    zs_est = np.array([zf_est(x,y) for x,y in zip(xs, ys)])
    Z_est = zs_est.reshape(X.shape)

    fig = plt.figure()
    ax = fig.gca(projection=&#39;3d&#39;)
    ax.plot_wireframe(X, Y, Z_est)

    for x,y,z in zip(xs, ys, zs[:]):
        ax.scatter(x,y,z, c=&#39;r&#39;, marker=&#39;.&#39;, s=20)

    ax.set_xlabel(&#39;x_1&#39;)
    ax.set_ylabel(&#39;x_2&#39;)
    ax.set_zlabel(&#39;z&#39;)

    plt.show()
#----------------------------------------------------------------

# Create data for training
create_data()

# Model
w_0 = tf.Variable(np.float32(np.random.rand()))
w = tf.Variable(np.float32(np.random.rand(1,2)))
h = tf.matmul(w, np.stack((xs,ys)) ) + w_0
loss = tf.reduce_mean(tf.square(h - zs))
optimizer = tf.train.GradientDescentOptimizer(0.0001)
train = optimizer.minimize(loss)

init = tf.initialize_all_variables()
sess=tf.InteractiveSession()
sess.run(init)

# Learning
step = 0
while 1:
    try:
        step += 1
        sess.run(train)
        if step % 100 == 0:
            print step, loss.eval(), w.eval()[0][0], w.eval()[0][1], w_0.eval()

    # Ctrl+c will stop training
    except KeyboardInterrupt:
        break

# Plot the result
draw(w.eval(), w_0.eval())

sess.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Below is what we can obtained from the code.

&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/linear_multiple.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;Linear regression ($n=2$)&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Optimization</title>
      <link>https://helix979.github.io/jkoo/post/ml-optimization/</link>
      <pubDate>Sun, 31 Jan 2016 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-optimization/</guid>
      <description>

&lt;p&gt;Machine learning often ends up with a mathematical optimization problem,
typically minimizing a cost function $J(\mathbf{w})$ for a given parameter $\mathbf{w} = \begin{bmatrix} w_0 &amp;amp; w_1 &amp;amp; \cdots &amp;amp; w_n \end{bmatrix}^{T} \in \mathbb{R}^{n+1}$ that has the form:&lt;/p&gt;

&lt;p&gt;$$
J(\mathbf{w})=\frac{1}{M} \sum_{m=1}^{M} J_m(\mathbf{w}).
\tag{opt:1}\label{eq:opt}
$$&lt;/p&gt;

&lt;p&gt;Here, $J_m(\mathbf{w})$ is the cost associated with the
$m$-th observation in a training set, and $M$ denotes the total number
of observations. Thus, one can say that given the parameter
$\mathbf{w}$, the cost function $J(\mathbf{w})$ represents the average
cost over all observations in the training set. Among all the possible
parameters, the one that results in the smallest value of the cost
function is called the &lt;strong&gt;minimizer, or solution&lt;/strong&gt;, and we denote it by $\mathbf{w}^{*}$.
The term &amp;ldquo;learning&amp;rdquo; in the machine learning means a
procedure for finding the solution $\mathbf{w}^{*}$.&lt;/p&gt;

&lt;h2 id=&#34;gradient-descent&#34;&gt;&lt;strong&gt;Gradient descent&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;Now, how can we find the solution $\mathbf{w}^{*}$ to \eqref{eq:opt}? One
simple way is the &lt;strong&gt;gradient descent&lt;/strong&gt;. The method of gradient descent
is to find a &lt;strong&gt;local minimum&lt;/strong&gt; of a cost function $J(\mathbf{w})$,
taking steps in the opposite direction to the gradient
$\nabla{J(\mathbf{w})}$, where&lt;/p&gt;

&lt;p&gt;$$
\nabla{J(\mathbf{w})})=
\begin{bmatrix}
\frac{\partial}{\partial w_0} J(\mathbf{w}) \\&lt;br /&gt;
\frac{\partial}{\partial w_1} J(\mathbf{w}) \\&lt;br /&gt;
\vdots \\&lt;br /&gt;
\frac{\partial}{\partial w_n} J(\mathbf{w})
\end{bmatrix}.
$$&lt;/p&gt;

&lt;p&gt;Consider the Taylor series of
$J(\mathbf{w}-\alpha \nabla{J(\mathbf{w})})$ as a function of $\alpha$:
$$J(\mathbf{w}-\alpha \nabla{J(\mathbf{w})})=J(\mathbf{w})-\alpha\Vert \nabla{J(\mathbf{w})}\Vert^2 + o(\alpha).$$
If $\nabla{J(\mathbf{w})} \ne 0$, then for sufficiently small
$\alpha&amp;gt;0$, we have
$$J(\mathbf{w}-\alpha \nabla{J(\mathbf{w})})&amp;lt;J(\mathbf{w}).$$ This means
that the parameter $\mathbf{w}-\alpha \nabla{J(\mathbf{w})}$ leads to
the smaller value of the cost function than the parameter $\mathbf{w}$.
Intuitively, the gradient $\nabla{J(\mathbf{w})}$ is the direction of
the maximum rate of increase at the parameter $\mathbf{w}$, so moving in
the opposite direction lowers the value of the cost function.&lt;/p&gt;

&lt;p&gt;Thus, we can start with a random guess $\mathbf{w}^{(0)}$ and keep
moving to $\mathbf{w}^{(1)}$, $\mathbf{w}^{(2)}$, $\ldots$,
$\mathbf{w}^{(t)}$ in such a way that
$$
\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)}-\alpha \nabla{J(\mathbf{w}^{(t)})},
\tag{opt:2}\label{eq:gd}
$$
or in other form
$$
w_{j}^{(t+1)} = w_{j}^{(t)}-\alpha \left. \frac{\partial J(\mathbf{w})}{\partial w_{j}} \right\vert_{\mathbf{w}=\mathbf{w}^{(t)}}\text{ for all } j.
\label{eq:gd_element}
$$
Then, for sufficiently small $\alpha&amp;gt;0$, we have, for every $t$,
$$J(\mathbf{w}^{(t+1)}) &amp;lt; J(\mathbf{w}^{(t)}),$$ and $\mathbf{w}^{(t)}$
converges to a local minimum as $t$ grows. When the cost function
$J(\mathbf{w})$ is convex, the local minimum is also the global minimum,
so in such a case, $\mathbf{w}^{(t)}$ can converge to the solution
$\mathbf{w}^{*}$. For non-convex cost functions, the gradient descent
does not guarantee us finding the global minimum. Instead, depending on
the initial guess, we will end up with different local minimum, which is
exemplified in Figure 1.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/gd_initial.png&#34; alt=&#34;The red and green dots represent the trajectories of the gradient descent corresponding to the initial values $w^{(0)}=-135$ and $w^{(0)}=30$, respectively.&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;The effect of the initial guess when $J(w)=(w&amp;#43;90)(w-50)(w/2&amp;#43;50)(w-90)/1000000$.&lt;/h4&gt;
        &lt;p&gt;
        The red and green dots represent the trajectories of the gradient descent corresponding to the initial values $w^{(0)}=-135$ and $w^{(0)}=30$, respectively.
        
            
        
        &lt;/p&gt; 
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;h2 id=&#34;learning-rate&#34;&gt;&lt;strong&gt;Learning rate&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;In \eqref{eq:gd}, the value of $\alpha$ is called the &lt;strong&gt;learning rate&lt;/strong&gt;. This value determines how fast or slow the parameter moves
towards the optimal solution. Figure 2 illustrates
the effect of the learning rate.&lt;/p&gt;


&lt;figure &gt;
    
        &lt;img src=&#34;https://helix979.github.io/jkoo/jkoo/img/gd_learning_rate.png&#34; /&gt;
    
    
    &lt;figcaption&gt;
        &lt;h4&gt;The effect of the learning rate when $J(w)=\frac{1}{2}w^2$ and $w^{(0)}=-20$.&lt;/h4&gt;
        
    &lt;/figcaption&gt;
    
&lt;/figure&gt;


&lt;p&gt;If $\alpha$ is too small, the gradient descent may take too long to converge. In contrast, the gradient descent
may fail to converge with too large value of $\alpha$.&lt;/p&gt;

&lt;p&gt;Choosing the proper value of the learning rate is not that trivial. In
practice, we often use simply a small enough constant by keeping
decreasing the value until the parameter seems to converge to a certain
point. Or in order to get more accurate solution, one can halve the
value of the learning rate as convergence slows down over iterations.
Another method is to adaptively change the learning rate at every
iteration $t$ in the way that the difference
$ J(\mathbf{w}^{(t)})-J(\mathbf{w}^{(t+1)})$ is maximized. For more
detail on this method, refer to the &lt;strong&gt;steepest descent&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;stochastic-gradient-descent&#34;&gt;&lt;strong&gt;Stochastic gradient descent&lt;/strong&gt;&lt;/h2&gt;

&lt;p&gt;From \eqref{eq:opt}, the gradient $\nabla{J(\mathbf{w})}$ is represented
as
$$
\nabla{J(\mathbf{w})}=\frac{1}{M} \sum_{m=1}^{M} \nabla{J_m(\mathbf{w})}.
\label{eq:gradient}
$$
 This implies that computing $\nabla{J(\mathbf{w})}$ is equivalent to taking the average of
$\nabla{J_m(\mathbf{w})}$, the gradient of the cost specific to the
$m$-th observation, over the full training set. However, in practice,
the training set is often very large, and thus averaging over the entire
set can take a significant time. For this reason, the &lt;strong&gt;stochastic
gradient descent&lt;/strong&gt; simply approximates the true gradient by a
gradient at a single observation as follows:
$$\nabla{J(\mathbf{w})}\approxeq\nabla{J_m(\mathbf{w})}.
\label{eq:gradient_approx}$$ Thus, with the stochastic gradient descent,
the parameter update equation in \eqref{eq:gd} can be rewritten as
$$
\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)}-\alpha \nabla{J_t(\mathbf{w}^{(t)})}.
\label{eq:sto_gd}
$$&lt;/p&gt;

&lt;p&gt;As a compromise between the true gradient and the gradient at a single
observation, one may also consider the gradient averaged over a few
training samples.&lt;/p&gt;

&lt;h3 id=&#34;practice .unnumbered&#34;&gt;&lt;strong&gt;Practice&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;Tensorflow provides a function that implements the gradient descent
algorithm. Thus, you can just use it without deriving an actual
gradient. What you need to do is to choose the value of the learning
rate. The following shows an example.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import tensorflow as tf
import numpy as np

w = tf.Variable(100.0)  # initial guess = 100.0
J = tf.pow(w, 2)        # J(w) = w^2

optimizer = tf.train.GradientDescentOptimizer(0.05)     # learning rate = 0.05
train = optimizer.minimize(J)

# Initialize variables
init = tf.initialize_all_variables()

sess = tf.InteractiveSession()
sess.run(init)

for step in range(0, 201):
    sess.run(train)
    if step % 20 == 0:
        print &amp;quot;%3d, %10.5f, %10.5f&amp;quot; % (step, w.eval(), J.eval())

sess.close()        
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The output of the script would be as follows:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  0,   90.00000, 8100.00000
 20,   10.94190,  119.72514
 40,    1.33028,    1.76964
 60,    0.16173,    0.02616
 80,    0.01966,    0.00039
100,    0.00239,    0.00001
120,    0.00029,    0.00000
140,    0.00004,    0.00000
160,    0.00000,    0.00000
180,    0.00000,    0.00000
200,    0.00000,    0.00000
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
