<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.31.1" />
  <meta name="author" content="Jinkyu Koo">
  <meta name="description" content="research scientist / dad / ice hockey player in dreams">

  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/jkoo/css/highlight.min.css">
    
  
  <link rel="stylesheet" href="/jkoo/css/bootstrap.min.css">
  <link rel="stylesheet" href="/jkoo/css/font-awesome.min.css">
  <link rel="stylesheet" href="/jkoo/css/academicons.min.css">
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700|Merriweather|Roboto+Mono">
  <link rel="stylesheet" href="/jkoo/css/hugo-academic.css">
  

  <link rel="alternate" href="https://helix979.github.io/jkoo/index.xml" type="application/rss+xml" title="Jinkyu Koo">
  <link rel="feed" href="https://helix979.github.io/jkoo/index.xml" type="application/rss+xml" title="Jinkyu Koo">

  <link rel="icon" type="image/png" href="/jkoo/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/jkoo/img/apple-touch-icon.png">

  <link rel="canonical" href="https://helix979.github.io/jkoo/post/ml-softmax/">

  

  <title>Softmax Regression | Jinkyu Koo</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/jkoo/">Jinkyu Koo</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/jkoo/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/jkoo/#pub">
            
            <span>Publications</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/jkoo/#patent">
            
            <span>US Patents</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/jkoo/#ml">
            
            <span>Machine Learning</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/jkoo/#misc">
            
            <span>Misc</span>
          </a>
        </li>

        
        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  

  <div class="article-container">
    <h1 itemprop="name">Softmax Regression</h1>
    

<div class="article-metadata">

  <span class="article-date">
    <time datetime="2016-02-11 23:59:55 -0500 EST" itemprop="datePublished">
      Thu, Feb 11, 2016
    </time>
  </span>

  
  
  
  

  
  
  
  <span class="article-tags">
    <i class="fa fa-tags"></i>
    
    <a href="/jkoo/tags/ml">ml</a
    >
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fhelix979.github.io%2fjkoo%2fpost%2fml-softmax%2f"
         target="_blank">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Softmax%20Regression&amp;url=https%3a%2f%2fhelix979.github.io%2fjkoo%2fpost%2fml-softmax%2f"
         target="_blank">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fhelix979.github.io%2fjkoo%2fpost%2fml-softmax%2f&amp;title=Softmax%20Regression"
         target="_blank">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fhelix979.github.io%2fjkoo%2fpost%2fml-softmax%2f&amp;title=Softmax%20Regression"
         target="_blank">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Softmax%20Regression&amp;body=https%3a%2f%2fhelix979.github.io%2fjkoo%2fpost%2fml-softmax%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>

    <div class="article-style" itemprop="articleBody">
      

<p>Softmax regression is a classification method that generalizes logistic
regression to multiclass problems in which possible outcomes are more
than two. For this reason, softmax regression is also called multinomial
logistic regression.</p>

<h3 id="generalization-from-logistic-regression">Generalization from logistic regression</h3>

<p>In logistic regression, we have modelled the conditional probabilities
as $$\begin{aligned}
P(y_{m}=1 \vert \mathbf{x}_{m})=\frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}_{m}}}
\end{aligned}
\tag{sm:1}\label{eq:cond1}
$$
 and
 $$\begin{aligned}
P(y_{m}=0 \vert \mathbf{x}_{m})=\frac{e^{-\mathbf{w}^T \mathbf{x}_{m}}}{1 + e^{-\mathbf{w}^T \mathbf{x}_{m}}},
\end{aligned}
\tag{sm:2}\label{eq:cond2}
$$
 which can be modified to
$$\begin{aligned}
P(y_{m}=1 \vert \mathbf{x}_{m})=\frac{e^{\mathbf{w}_1^T \mathbf{x}_{m}}}{e^{\mathbf{w}_1^T \mathbf{x}_{m}} + e^{(\mathbf{w}_1-\mathbf{w})^T \mathbf{x}_{m}}}
\end{aligned}
\tag{sm:3}\label{eq:cond3}
$$
 and
 $$\begin{aligned}
P(y_{m}=0 \vert \mathbf{x}_{m})=\frac{e^{(\mathbf{w}_1-\mathbf{w})^T \mathbf{x}_{m}}}{e^{\mathbf{w}_1^T \mathbf{x}_{m}} + e^{(\mathbf{w}_1-\mathbf{w})^T \mathbf{x}_{m}}}
\end{aligned}
\tag{sm:4}\label{eq:cond4}
$$</p>

<p>Note that \eqref{eq:cond3} and
\eqref{eq:cond4} are made by multiplying
$\frac{e^{\mathbf{w}_1^T \mathbf{x}_{m}}}{e^{\mathbf{w}_1^T \mathbf{x}_{m}}}$
to \eqref{eq:cond1} and \eqref{eq:cond2}, respectively. Letting
$\mathbf{w}_0=\mathbf{w}_1-\mathbf{w}$, we can rewrite \eqref{eq:cond3}
and \eqref{eq:cond4} as follows: $$\begin{aligned}
P(y_{m}=k \vert \mathbf{x}_{m})=\frac{e^{\mathbf{w}_k^T \mathbf{x}_{m}}}{\sum_{i=0}^{1} e^{\mathbf{w}_i^T \mathbf{x}_{m}}} \text{ for } k=0,1.
\label{eq:cond5}\end{aligned}$$ Thus, the conditional probabilities can
be thought of as the ones that exponentiate weighted inputs and then
normalize them. In such a sense, softmax regression considers more than
two classes, say $K$ of them like $y_{m} \in {1,2,\ldots,K}$, with the
model of conditional probabilities in the following: $$\begin{aligned}
P(y_{m}=k \vert \mathbf{x}_{m})=\frac{e^{\mathbf{w}_k^T \mathbf{x}_{m}}}{\sum_{i=1}^{K} e^{\mathbf{w}_i^T \mathbf{x}_{m}}} \text{ for } k=1,2,\ldots,K.
\label{eq:cond6}\end{aligned}$$ Here,
$\mathbf{w}_1, \mathbf{w}_2, \ldots, \mathbf{w}_K \in \mathbb{R}^{n+1}$
are the parameter vectors of our model.</p>

<h3 id="hypothesis-function">Hypothesis function</h3>

<p>Let a parameter matrix $\mathbf{W} \in \mathbb{R}^{(n+1) \times K}$ be</p>

<p>$$
\begin{aligned}
\mathbf{W} =
\begin{bmatrix} \mathbf{w}_1 &amp; \mathbf{w}_2 &amp; \cdots &amp; \mathbf{w}_K
\end{bmatrix}.
\end{aligned}
$$</p>

<p>Then, we define the hypothesis function of softmax regression, denoted
by $\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m}) \in \mathbb{R}^{K}$, as
$$\begin{aligned}
\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m}) =
\begin{bmatrix}
P(y_{m}=1 \vert \mathbf{x}_{m}) \\<br />
P(y_{m}=2 \vert \mathbf{x}_{m}) \\<br />
\vdots \\<br />
P(y_{m}=K \vert \mathbf{x}_{m}) \\<br />
\end{bmatrix}
= \frac{1}{\sum_{i=1}^{K} e^{\mathbf{w}_i^T \mathbf{x}_{m}}}
\begin{bmatrix}
e^{\mathbf{w}_1^T \mathbf{x}_{m}} \\<br />
e^{\mathbf{w}_2^T \mathbf{x}_{m}} \\<br />
\vdots \\<br />
e^{\mathbf{w}_k^T \mathbf{x}_{m}} \\<br />
\end{bmatrix}.\end{aligned}$$ The parameter matrix $\mathbf{W}$ should
be chosen in such a way that for $y_{m}=k$, the $k$-th element of
$\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m})$ becomes the largest among all
the elements.</p>

<p>Note that $\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m})$ can be represented
as $$\begin{aligned}
\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m}) =
\sigma(\mathbf{W}^{T} \mathbf{x}_{m}),\end{aligned}$$ where
$\sigma(\mathbf{z})$ for
$\mathbf{z} = \begin{bmatrix} z_1 &amp; z_2 &amp; \ldots &amp; z_K \end{bmatrix}^{T} \in \mathbb{R}^{K}$
is called the <strong>softmax function</strong> and is given as $$\begin{aligned}
\sigma(\mathbf{z})=
\begin{bmatrix}
\sigma(\mathbf{z};1) \\<br />
\sigma(\mathbf{z};2) \\<br />
\vdots \\<br />
\sigma(\mathbf{z};K) \\<br />
\end{bmatrix}\end{aligned}$$ with $$\begin{aligned}
\sigma(\mathbf{z};i) = \frac{e^{z_i}}{\sum_{k=1}^{K} e^{z_k}}.
\end{aligned}\tag{sm:5}\label{eq:sm_sigma}$$ The derivatives of the softmax
function have a nice property to remember:</p>

<p>$$\begin{aligned}
\frac{\partial \sigma(\mathbf{z};i)}{\partial z_j}=
\left\{
\begin{matrix}
\sigma(\mathbf{z};i) (1 - \sigma(\mathbf{z};i)) &amp; \text{if } i=j,\\<br />
-\sigma(\mathbf{z};i)\sigma(\mathbf{z};j) &amp; \text{if } i \neq j.\\<br />
\end{matrix} \right.\end{aligned}
$$</p>

<h3 id="cost-function">Cost function</h3>

<p>The likelihood $l(\mathbf{W})$ of all data samples in the training set
can be represented as follows: $$\begin{aligned}
l(\mathbf{W})=\prod_{m=1}^{M}\prod_{k=1}^{K} \left( \frac{e^{\mathbf{w}_k^T \mathbf{x}_{m}}}{\sum_{i=1}^{K} e^{\mathbf{w}_i^T \mathbf{x}_{m}}} \right)^{\mathbf{1}_{m}(k)},
\label{eq:sm_likelihood}\end{aligned}$$ where the indicator function
$\mathbf{1}_{m}(k)$ is the same as <a href="/jkoo/post/ml-logistic/#mjx-eqn-eqindicator" target="_blank">(logi:6)</a>. Softmax
regression attempts to find the parameter matrix $\mathbf{W}$ that
maximizes the likelihood $l(\mathbf{W})$. This causes the $k$-th element
of $\mathbf{h}_{\mathbf{W}}(\mathbf{x}_{m})$ to be larger than any other
when $y_{m}=k$. Towards this end, the cost function is defined in a form
of a negative log-likelihood, as in logistic regression:</p>

<p>$$
\begin{aligned}
J(\mathbf{W})
&amp;= -\frac{1}{M}\log l(\mathbf{W}) \\<br />
&amp;= -\frac{1}{M}\sum_{m=1}^{M} \sum_{k=1}^{K}
{\mathbf{1}_{m}(k)} \log\left( \frac{e^{\mathbf{w}_k^T \mathbf{x}_{m}}}{\sum_{i=1}^{K} e^{\mathbf{w}_i^T \mathbf{x}_{m}}} \right).
\end{aligned}\tag{sm:6}\label{eq:sm_cost}
$$</p>

<p>It is worth noting that when $K=2$,
\eqref{eq:sm_cost} is equivalent to <a href="/jkoo/post/ml-logistic/#mjx-eqn-eqlogistic_cost_multi" target="_blank">(logi:5)</a>.</p>

<h3 id="learning">Learning</h3>

<p>There is no known closed-form solution that minimizes the cost function
$J(\mathbf{W})$ in \eqref{eq:sm_cost}. Thus, we can apply the gradient
descent here as well. Using the notation in \eqref{eq:sm_sigma}, it is
easy to see:
$$\small
\begin{aligned}
\frac{\partial J(\mathbf{W})}{\partial \mathbf{w}_j}=
-\frac{1}{M}\sum_{m=1}^{M} \sum_{k=1}^{K}
\mathbf{1}_{m}(k) \left(\frac{1}{\sigma(\mathbf{W}^T \mathbf{x}_{m};k)} \cdot \frac{\partial \sigma(\mathbf{W}^T \mathbf{x}_{m};k)}{\partial (\mathbf{w}_j^T \mathbf{x}_{m})} \right) \mathbf{x}_{m}
\end{aligned}
$$</p>

<p>Focusing on the difference between the term when $k=j$ and the terms
when $k \neq j$ of the inner summation, we continue to develop the
derivative equation as follows:</p>

<p>$$\tiny
\begin{aligned}
\frac{\partial J(\mathbf{W})}{\partial \mathbf{w}_j}
&amp;=-\frac{1}{M}\sum_{m=1}^{M} \left( \mathbf{1}_{m}(j)(1-\sigma(\mathbf{W}^T \mathbf{x}_{m};j))-\sum_{k \neq j} \mathbf{1}_{m}(k)\sigma(\mathbf{W}^T \mathbf{x}_{m};j) \right) \mathbf{x}_{m} \\<br />
&amp;=-\frac{1}{M}\sum_{m=1}^{M} \left( \mathbf{1}_{m}(j)-\left(\sum_{k=1}^{K} \mathbf{1}_{m}(k) \right)\sigma(\mathbf{W}^T \mathbf{x}_{m};j) \right) \mathbf{x}_{m} \\<br />
&amp;=-\frac{1}{M}\sum_{m=1}^{M} \left( \mathbf{1}_{m}(j)-\sigma(\mathbf{W}^T \mathbf{x}_{m};j) \right) \mathbf{x}_{m}.
\end{aligned}
$$</p>

<p>Therefore, the gradient descent equations of softmax regression are
given by $$\begin{aligned}
\mathbf{w}_{j}^{(t+1)}=\mathbf{w}_{j}^{(t)}-\frac{\alpha}{M}\sum_{m=1}^{M} \left( \mathbf{1}_{m}(j)-\sigma(\mathbf{W}^{(t)T} \mathbf{x}_{m};j) \right) \mathbf{x}_{m}
\end{aligned}
$$
for all $j$.</p>

<h3 id="prediction">Prediction</h3>

<p>Denoting a new input vector by
$\mathbf{x} = \begin{bmatrix} 1 &amp; x_1 &amp; \ldots &amp; x_n \end{bmatrix}^{T}$
and the solution by $\mathbf{W}^{*} =
\begin{bmatrix} \mathbf{w}_1^{*} &amp; \mathbf{w}_2^{*} &amp; \cdots &amp; \mathbf{w}_K^{*} \end{bmatrix}$,
we decide $y=k$ when the largest element of
$\mathbf{h}_{\mathbf{W}^{*}}(\mathbf{x})$ is the $k$-th. In other words,
the prediction rule is given by: $$\begin{aligned}
y=\arg\max_{k}\left( \frac{e^{\mathbf{w}_k^{*T} \mathbf{x}}}{\sum_{i=1}^{K} e^{\mathbf{w}_i^{*T} \mathbf{x}}} \right).\end{aligned}$$</p>

<h3 id="practice">Practice</h3>

<pre><code>import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import random
import sys


n_data = 10000
r_x = 10
n_class = 3
n_batch = 10

#---------------------------------------------------------------------
def create_data(n):
    &quot;&quot;&quot;
    This function will make a set of data such that
    a random number between c*r_x and (c+1)*r_x is given a label c. 
    &quot;&quot;&quot;
    dataset = []
    for i in range(n):
        c = np.random.randint(n_class)
        x_1 = np.random.rand() * r_x + c*r_x
        y = c
        sample = [x_1, y]
        dataset.append(sample)
    random.shuffle(dataset)
    point_, label_ = zip(*dataset)
    _point_ = np.float32(np.array([point_]))
    _label_ = np.zeros([n_class, n])
    for i in range(len(label_)):
        _label_[label_[i]][i] = 1
    return _point_, _label_
#---------------------------------------------------------------------

# Create a dataset for training
point, label = create_data(n_data)

# Placeholders to take data in
x = tf.placeholder(tf.float32, [1, None])
y = tf.placeholder(tf.float32, [n_class, None])

# Write a model
w_1 = tf.Variable(tf.random_uniform([n_class, 1], -1.0, 1.0))
w_0 = tf.Variable(tf.random_uniform([n_class, 1], -1.0, 1.0))
y_hat = tf.nn.softmax(w_1 * x + w_0)
cost = -tf.reduce_sum(y*tf.log(y_hat))/n_batch
optimizer = tf.train.GradientDescentOptimizer(0.001)
train = optimizer.minimize(cost)

label_hat_ = tf.argmax(y_hat,0)
correct_cnt = tf.equal(tf.argmax(y,0), label_hat_)
accuracy = tf.reduce_mean(tf.cast(correct_cnt, &quot;float&quot;))


sess = tf.InteractiveSession()

# Initialize variables
init = tf.initialize_all_variables()
sess.run(init)

# Learning
step = 0
while 1:
    try:
        step += 1
        if (n_data == n_batch): # Gradient descent
            start = 0
            end = n_data
        else: # Stochastic gradient descent
            start = step * n_batch
            end = start + n_batch
            if (start &gt;=n_data) or (end &gt;=n_data):
                break
        batch_xs = point[:, start:end]
        batch_ys = label[:, start:end]

        train.run(feed_dict={x: batch_xs, y: batch_ys})

        if step % 10 == 0:
            print step
            print w_1.eval()
            print w_0.eval()

    # With gradient descent, ctrl+c will stop training
    except KeyboardInterrupt:
        break


# Create another dataset for test
point_t, label_t = create_data(100)
rate = accuracy.eval(feed_dict={x: point_t, y: label_t})
print &quot;\n\n accuracy = %s\n&quot; % (rate)

# Plot the test results
plt.plot(point_t[0,:], label_hat_.eval(feed_dict={x: point_t}), 'o')
plt.grid()
plt.ylim(-1, n_class)

xt = range(0, n_class*10+1, 10)
yt = range(-1, n_class, 1)
plt.step(xt, yt, 'r--')

plt.savefig('softmax_test.pdf')

sess.close()
</code></pre>

<p>Figure 1 is what we may get from the code above.</p>


<figure >
    
        <img src="/jkoo/img/softmax_test.png" />
    
    
    <figcaption>
        <h4>Blue dots that are not on the red line indicate classification errors.</h4>
        
    </figcaption>
    
</figure>


    </div>
  </div>

</article>

<div class="container">
  <nav>
  <ul class="pager">
    
    <li class="previous"><a href="https://helix979.github.io/jkoo/post/ml-logistic/"><span
      aria-hidden="true">&larr;</span> Logistic Regression</a></li>
    

    
    <li class="next"><a href="https://helix979.github.io/jkoo/post/ml-svm/">Support Vector Machines <span
      aria-hidden="true">&rarr;</span></a></li>
    
  </ul>
</nav>

</div>

<div class="article-container">
  

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2017 Jinkyu Koo &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
    <script src="/jkoo/js/jquery-1.12.3.min.js"></script>
    <script src="/jkoo/js/bootstrap.min.js"></script>
    <script src="/jkoo/js/isotope.pkgd.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.1/imagesloaded.pkgd.min.js"></script>
    <script src="/jkoo/js/hugo-academic.js"></script>
    

    
    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-91702722-1', 'auto');
        ga('send', 'pageview');

         
        var links = document.querySelectorAll('a');
        Array.prototype.map.call(links, function(item) {
            if (item.host != document.location.host) {
                item.addEventListener('click', function() {
                    var action = item.getAttribute('data-action') || 'follow';
                    ga('send', 'event', 'outbound', action, item.href);
                });
            }
        });
    </script>
    

    
    
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
            TeX: { equationNumbers: { autoNumber: "AMS" },
                   extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
                   noErrors: {
                        multiLine: true
                   }
            },        
            tex2jax: {
                inlineMath: [['$','$'], ['\\(','\\)']],
                displayMath: [['$$','$$'], ['\[','\]']]
            }
        });
    </script>
    <script async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML"></script>
    

  </body>
</html>

