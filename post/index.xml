<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Jinkyu Koo</title>
    <link>https://helix979.github.io/jkoo/post/</link>
    <description>Recent content in Posts on Jinkyu Koo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Jinkyu Koo</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://helix979.github.io/jkoo/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Artificial Neural Networks</title>
      <link>https://helix979.github.io/jkoo/post/ml-ann/</link>
      <pubDate>Tue, 16 Feb 2016 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-ann/</guid>
      <description>An artificial neural network (ANN) is a machine learning model inspired by biological neural networks in a human brain. ANNs offer an elegant way to formulate a complex non-linear hypothesis, due to their hierarchical virtue of layering non-linear units. Even though possible, using an ANN for regression is considered overkill in many cases. Thus, our discussion will be focused on the use for classification.
Perceptrons A neuron, illustrated in Figure 1, is a core component of a human brain that transmits electrical signals from one place to others.</description>
    </item>
    
    <item>
      <title>Support Vector Machines</title>
      <link>https://helix979.github.io/jkoo/post/ml-svm/</link>
      <pubDate>Sun, 14 Feb 2016 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-svm/</guid>
      <description>Support vector machine (SVM) is a learning model based on the concept of a separating hyperplane that defines the decision boundary. SVM can be used for both classification and regression analysis, but here we explain it focusing on classification.
Separating hyperplane of the maximum margin Consider a training set shown in Figure 1, where data samples belong to one of two classes. The separating hyperplane is the decision boundary that divides the two classes by a hyperplane (a line in two dimensions, a plane in three dimensions, and so on).</description>
    </item>
    
    <item>
      <title>Softmax Regression</title>
      <link>https://helix979.github.io/jkoo/post/ml-softmax/</link>
      <pubDate>Thu, 11 Feb 2016 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-softmax/</guid>
      <description>Softmax regression is a classification method that generalizes logistic regression to multiclass problems in which possible outcomes are more than two. For this reason, softmax regression is also called multinomial logistic regression.
Generalization from logistic regression In logistic regression, we have modelled the conditional probabilities as $$\begin{aligned} P(y_{m}=1 \vert \mathbf{x}_{m})=\frac{1}{1 + e^{-\mathbf{w}^T \mathbf{x}_{m}}} \end{aligned} \tag{sm:1}\label{eq:cond1} $$ and $$\begin{aligned} P(y_{m}=0 \vert \mathbf{x}_{m})=\frac{e^{-\mathbf{w}^T \mathbf{x}_{m}}}{1 + e^{-\mathbf{w}^T \mathbf{x}_{m}}}, \end{aligned} \tag{sm:2}\label{eq:cond2} $$ which can be modified to $$\begin{aligned} P(y_{m}=1 \vert \mathbf{x}_{m})=\frac{e^{\mathbf{w}_1^T \mathbf{x}_{m}}}{e^{\mathbf{w}_1^T \mathbf{x}_{m}} + e^{(\mathbf{w}_1-\mathbf{w})^T \mathbf{x}_{m}}} \end{aligned} \tag{sm:3}\label{eq:cond3} $$ and $$\begin{aligned} P(y_{m}=0 \vert \mathbf{x}_{m})=\frac{e^{(\mathbf{w}_1-\mathbf{w})^T \mathbf{x}_{m}}}{e^{\mathbf{w}_1^T \mathbf{x}_{m}} + e^{(\mathbf{w}_1-\mathbf{w})^T \mathbf{x}_{m}}} \end{aligned} \tag{sm:4}\label{eq:cond4} $$</description>
    </item>
    
    <item>
      <title>Logistic Regression</title>
      <link>https://helix979.github.io/jkoo/post/ml-logistic/</link>
      <pubDate>Tue, 09 Feb 2016 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-logistic/</guid>
      <description>Logistic regression is a classification method for analyzing a set of data in which there are one or more independent variables (inputs) that determine a binary outcome $0$ or $1$.
Hypothesis function Suppose that we are given $M$ data samples, denoting the $m$-th observation pair of an input vector and an output by $\mathbf{x}_{m} = \begin{bmatrix} x_0 &amp;amp; x_1 &amp;amp; \cdots &amp;amp; x_n \end{bmatrix}^{T} \in \mathbb{R}^{n+1}$ and $y_{m} \in {0,1}$, respectively.</description>
    </item>
    
    <item>
      <title>Linear Regression</title>
      <link>https://helix979.github.io/jkoo/post/ml-linear_regression/</link>
      <pubDate>Tue, 02 Feb 2016 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-linear_regression/</guid>
      <description>Linear regression is a means of modeling the relationship between one or more independent variables (inputs) and a single dependent variable (an output) by fitting a linear equation to observed data.

Hypothesis function Given the $m$-th observation with inputs $\{ x_{m,1}, x_{m,2}, \ldots, x_{m,n} \}$ where $x_{m,j} \in \mathbb{R}, \forall j$ and an output $y_m \in \mathbb{R}$, we define an input vector as
$$ \mathbf{x}_m= \begin{bmatrix} x_{m,0} \\</description>
    </item>
    
    <item>
      <title>Optimization</title>
      <link>https://helix979.github.io/jkoo/post/ml-optimization/</link>
      <pubDate>Sun, 31 Jan 2016 23:59:55 -0500</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/ml-optimization/</guid>
      <description>Machine learning often ends up with a mathematical optimization problem, typically minimizing a cost function $J(\mathbf{w})$ for a given parameter $\mathbf{w} = \begin{bmatrix} w_0 &amp;amp; w_1 &amp;amp; \cdots &amp;amp; w_n \end{bmatrix}^{T} \in \mathbb{R}^{n+1}$ that has the form:
$$ J(\mathbf{w})=\frac{1}{M} \sum_{m=1}^{M} J_m(\mathbf{w}). \tag{opt:1}\label{eq:opt} $$
Here, $J_m(\mathbf{w})$ is the cost associated with the $m$-th observation in a training set, and $M$ denotes the total number of observations. Thus, one can say that given the parameter $\mathbf{w}$, the cost function $J(\mathbf{w})$ represents the average cost over all observations in the training set.</description>
    </item>
    
    <item>
      <title>Python in half an hour</title>
      <link>https://helix979.github.io/jkoo/post/python-tutorial/</link>
      <pubDate>Sun, 29 Nov 2015 11:00:00 +0000</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/python-tutorial/</guid>
      <description>In what follows in this tutorial, the result of a statement is expressed in the form of #result#, unless otherwise mentioned.
 Basic arithmetic operations
# Comments start with a hash sign (#). 1 + 2 # Addition 1 / 2 #0# Division 1 / 2. #0.5# If either one of the numbers in a division is a float, # so does the result. 3 % 2 #1# Modulus 2 ** 3 #8# Power.</description>
    </item>
    
    <item>
      <title>Linux kernel scheduler</title>
      <link>https://helix979.github.io/jkoo/post/os-scheduler/</link>
      <pubDate>Mon, 05 Jan 2015 11:00:00 +0000</pubDate>
      
      <guid>https://helix979.github.io/jkoo/post/os-scheduler/</guid>
      <description>What is the kernel? The kernel is fundamental part of an operating system (OS) that manages the computerâ€™s hardwares, and allows softwares to run and use hardware resources in shared manners. Typically, the hardware resources to take into account are: (1) processors, (2) memory, and (3) input/output (I/O) devices such as keyboard, disk drives, network interface cards, and so on.
Rough distinction between an OS and a kernel is that an OS is the kernel plus some useful utilities and applications such as administration tools and GUIs.</description>
    </item>
    
  </channel>
</rss>